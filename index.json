[{"authors":["chuanguo"],"categories":null,"content":"Chuan Guo is a Phd student under the supervision of Dr.Li Cheng. His work now focuses on modeling human motions, such as multi-modal human motion generation. He had two-year research experience in the Multimedia Group at Institute of Computing Technology, Chinese Academy of Sciences. Before that, he recieved his bachelor degree from Software Engineering College at Jilin University. For more information, please visit his personal webpage.\n","date":1627257600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1627257600,"objectID":"142ed72b86ee96c93fea999fe151f5c6","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/chuan-guo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chuan-guo/","section":"authors","summary":"Chuan Guo is a Phd student under the supervision of Dr.Li Cheng. His work now focuses on modeling human motions, such as multi-modal human motion generation. He had two-year research experience in the Multimedia Group at Institute of Computing Technology, Chinese Academy of Sciences.","tags":null,"title":"Chuan Guo","type":"authors"},{"authors":["licheng"],"categories":null,"content":"I am an associate professor with the Department of Electrical and Computer Engineering, University of Alberta. Prior to joining University of Alberta in year 2018, I worked at A*STAR, Singapore, TTI-Chicago, USA, and NICTA, Australia. I received my BSc degree in Computer Science from Jilin University in 1996, M. Eng. degree from Nankai University in 1999, and PhD in Computing Science from the University of Alberta in 2004. My research expertise is mainly on computer vision and machine learning.\n","date":1627257600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1627257600,"objectID":"35904e6911f33bff2955e50d3ec791a7","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/li-cheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/li-cheng/","section":"authors","summary":"I am an associate professor with the Department of Electrical and Computer Engineering, University of Alberta. Prior to joining University of Alberta in year 2018, I worked at A*STAR, Singapore, TTI-Chicago, USA, and NICTA, Australia.","tags":null,"title":"Li Cheng","type":"authors"},{"authors":["senwang"],"categories":null,"content":"Sen Wang is a Postdoctoral Fellow under the supervision of Dr. Li Cheng at University of Alberta and Dr. Minglun Gong at University of Guelph. He have graduated from the Northwestern Polytechnical University with his Ph.D degree in 2019, advised by Dr. Runxiao Wang. Before that, he recieved my B.Eng. degree from the School of Mechanical Engineering from Northwestern Polytechnical University in 2011. During 2015-2016, he was a Visiting Ph.D Student at University of Kentucky, advised by Dr. Ruigang Yang. His research interests include Computer Vision and Robotics, in particular in Motion Retargeting and Human Modeling.\nMy personal website can be visited here.\n","date":1627257600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1627257600,"objectID":"3626cece87f3467111c0fcd63a1ed522","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/sen-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sen-wang/","section":"authors","summary":"Sen Wang is a Postdoctoral Fellow under the supervision of Dr. Li Cheng at University of Alberta and Dr. Minglun Gong at University of Guelph. He have graduated from the Northwestern Polytechnical University with his Ph.","tags":null,"title":"Sen Wang","type":"authors"},{"authors":["shihaozou"],"categories":null,"content":"I am a second-year PhD student in the department of Electrical and Computer Engineering, University of Alberta, Canada. I am from Nanchang, China. My research interest includes computer vision (CV) and reinforcement learning (RL). I also participated in projects on multi-agent RL and some data mining topics, including information retrieval and natural language processing. My homepage\n","date":1627257600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1627257600,"objectID":"f8275347c56279c737988751172c0c96","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/shihao-zou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shihao-zou/","section":"authors","summary":"I am a second-year PhD student in the department of Electrical and Computer Engineering, University of Alberta, Canada. I am from Nanchang, China. My research interest includes computer vision (CV) and reinforcement learning (RL).","tags":null,"title":"Shihao Zou","type":"authors"},{"authors":["shuangwu"],"categories":null,"content":"Chuan Guo is a PhD student at the Vision and Learning Lab. His research interests include computer vision and machine learning.\n","date":1627257600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1627257600,"objectID":"2f44b728c95bf3cf134217ac2f94af6f","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/shuang-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shuang-wu/","section":"authors","summary":"Chuan Guo is a PhD student at the Vision and Learning Lab. His research interests include computer vision and machine learning.","tags":null,"title":"Shuang Wu","type":"authors"},{"authors":["tk"],"categories":null,"content":"I am a masters student at the University of Alberta with an interest in Machine Learning, and Computer Vision. I obtained my BSc from Tel Aviv University in 2019, where I worked with Professor Nachum Dershowitz on various Digital Humanities projects (see publications). In 2019, I joined the University of Alberta as a masters student, supervised by Professor Li Cheng. I am currently working on medical report generation, and video summarization. When I am not working, I like running, making jokes, and playing video games (usually not simultaneously).\n Past publications before my master study：\n  Badamdorj, Taivanbat, Adiel Ben-Shalom, and Nachum Dershowitz. \u0026ldquo;Matching and Searching the Dead Sea Scrolls.\u0026rdquo; 2018 IEEE International Conference on the Science of Electrical Engineering in Israel (ICSEE). IEEE, 2018.\n  Badamdorj, Taivanbat and Ben-Shalom, Adiel and Dershowitz, Nachum and Wolf, Lior. \u0026ldquo;Fast Search with Poor OCR\u0026rdquo; Abstracts of Digital Humanities. DH 2020, Ottawa, Canada\n  ","date":1627257600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1627257600,"objectID":"fb75d4c2070e898f9491360f071501f5","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/taivanbat-badamdorj/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/taivanbat-badamdorj/","section":"authors","summary":"I am a masters student at the University of Alberta with an interest in Machine Learning, and Computer Vision. I obtained my BSc from Tel Aviv University in 2019, where I worked with Professor Nachum Dershowitz on various Digital Humanities projects (see publications).","tags":null,"title":"Taivanbat Badamdorj","type":"authors"},{"authors":["xinxinzuo"],"categories":null,"content":"Xinxin Zuo is Postdoctoral Fellow under the supervision of Prof. Li Cheng at University of Alberta and Prof. Minglun Gong at University of Guelph. She have graduated from the University of Kentucky with her Ph.D degree in 2019, advised by Prof. Ruigang Yang. Before that, She recieved my B.Eng. and M.Eng. degree from the School of Computer Science at Northwestern Polytechnical University in 2011 and 2014. Her research interests include Computer Vision and Computer Graphics, in particular in 3D Reconstruction and Human Modeling.\nMy personal website can be visited here.\n","date":1627257600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1627257600,"objectID":"13020e93f889b0f7b71da0f13efdf970","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/xinxin-zuo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xinxin-zuo/","section":"authors","summary":"Xinxin Zuo is Postdoctoral Fellow under the supervision of Prof. Li Cheng at University of Alberta and Prof. Minglun Gong at University of Guelph. She have graduated from the University of Kentucky with her Ph.","tags":null,"title":"Xinxin Zuo","type":"authors"},{"authors":["jingjingli"],"categories":null,"content":"Jingjing Li is a PhD student under the supervision of Prof. Li Cheng at the Vision and Learning Lab, University of Alberta. Her work now focuses on Visual Saliency Analysis, Human Activity Understanding, and Medical Image Processing. She received the M.Sc. degree and B.S. degree from Dalian University of Technology, China, in 2019 and 2017, respectively.\n","date":1615852800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615852800,"objectID":"0877e4db656bd39107101fc25759b677","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/jingjing-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingjing-li/","section":"authors","summary":"Jingjing Li is a PhD student under the supervision of Prof. Li Cheng at the Vision and Learning Lab, University of Alberta. Her work now focuses on Visual Saliency Analysis, Human Activity Understanding, and Medical Image Processing.","tags":null,"title":"Jingjing Li","type":"authors"},{"authors":["weiji"],"categories":null,"content":"Wei Ji is a PhD student under the supervision of Prof. Li Cheng. His work focuses on designing efficient computer vision algorithms for 3D scene understanding, visual saliency analysis, and medical image processing. For more information, please visit his personal webpage.\n","date":1615852800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615852800,"objectID":"56d1cb33564a4b777ca985617511e538","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/wei-ji/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wei-ji/","section":"authors","summary":"Wei Ji is a PhD student under the supervision of Prof. Li Cheng. His work focuses on designing efficient computer vision algorithms for 3D scene understanding, visual saliency analysis, and medical image processing.","tags":null,"title":"Wei Ji","type":"authors"},{"authors":["mojtabas"],"categories":null,"content":"The focus of my research is on visual object tracking.\n Digital Image \u0026amp; Video Processing Lab (DIVPL), Yazd University, Iran Image Processing Lab (IPL), Sharif University of Technology, http://ipl.ce.sharif.edu/members.html  ","date":1606435200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1606435200,"objectID":"8fca144910d17d7118b4cd503093d653","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/seyed-mojtaba-marvasti-zadeh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/seyed-mojtaba-marvasti-zadeh/","section":"authors","summary":"The focus of my research is on visual object tracking.\n Digital Image \u0026amp; Video Processing Lab (DIVPL), Yazd University, Iran Image Processing Lab (IPL), Sharif University of Technology, http://ipl.ce.sharif.edu/members.html  ","tags":null,"title":"Seyed Mojtaba Marvasti-Zadeh","type":"authors"},{"authors":["mahdiarn"],"categories":null,"content":"","date":1592524800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592524800,"objectID":"9adc0dc4aaccbc7d876db03b03c3743c","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/mahdiar-nekoui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mahdiar-nekoui/","section":"authors","summary":"","tags":null,"title":"Mahdiar Nekoui","type":"authors"},{"authors":["jiyang"],"categories":null,"content":"Ji Yang is a PhD student with the Vision and Learning Lab at the Department of Electrical and Computer Engineering, University of Alberta. He received both his BSc Honors (2017) and MSc degree (2019) at the Department of Computing Science, University of Alberta. His research interests are mainly on computer vision, human hehavior analysis and machine learning.\n","date":1590969600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1590969600,"objectID":"a13dc716b1fa7fd3a82b8b575972f71c","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/ji-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ji-yang/","section":"authors","summary":"Ji Yang is a PhD student with the Vision and Learning Lab at the Department of Electrical and Computer Engineering, University of Alberta. He received both his BSc Honors (2017) and MSc degree (2019) at the Department of Computing Science, University of Alberta.","tags":null,"title":"Ji Yang","type":"authors"},{"authors":["heyan"],"categories":null,"content":"He Yan received the master’s degree from Nanjing Forestry University, Jiangsu, China, in 2014. Currently, he is pursuing the Ph.D. degree at Nanjing University of Science and Technology. His research interests include machine learning, pattern recognition, intelligent transportation systems, and their applications.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e2059829264e82c23e6a3f38db343bcc","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/he-yan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/he-yan/","section":"authors","summary":"He Yan received the master’s degree from Nanjing Forestry University, Jiangsu, China, in 2014. Currently, he is pursuing the Ph.D. degree at Nanjing University of Science and Technology. His research interests include machine learning, pattern recognition, intelligent transportation systems, and their applications.","tags":null,"title":"He Yan","type":"authors"},{"authors":["khaghanijavad"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f4087ff9395c74f9a37656ba3bb863e9","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/javad-khaghani/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/javad-khaghani/","section":"authors","summary":"","tags":null,"title":"Javad Khaghani","type":"authors"},{"authors":["youdongma"],"categories":null,"content":"Youdong Ma obtained his BSc in Computing Science from University of Alberta in 2016. He is a first-year master student with Vision and Learning Lab at the Department of Electrical and Computer Engineering, University of Alberta. His research interest is computer vision and human behavior analysis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2587919ac91f367247b9f9de2fd93580","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/youdong-ma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/youdong-ma/","section":"authors","summary":"Youdong Ma obtained his BSc in Computing Science from University of Alberta in 2016. He is a first-year master student with Vision and Learning Lab at the Department of Electrical and Computer Engineering, University of Alberta.","tags":null,"title":"Youdong Ma","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]]\rname = \u0026quot;Courses\u0026quot;\rurl = \u0026quot;courses/\u0026quot;\rweight = 50\r Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]]\rname = \u0026quot;Docs\u0026quot;\rurl = \u0026quot;docs/\u0026quot;\rweight = 50\r Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://vision-and-learning-lab-ualberta.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://vision-and-learning-lab-ualberta.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://vision-and-learning-lab-ualberta.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":["Shuang Wu","Zhenguang Liu","Shijian Lu","Li Cheng"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"2de0a8c9c0ced2928e9b2fe1e3f0d383","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/shuang_acmmm2021/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/post/shuang_acmmm2021/","section":"post","summary":" ","tags":null,"title":"Our paper \"Dual Learning Music Composition and Dance Choreography\" is accepted by ACM Multimedia 2021","type":"post"},{"authors":["Shihao Zou","Chuan Guo","Xinxin Zuo","Sen Wang","Pengyu Wang","Xiaoqin Hu","Shoushun Chen","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"2c8b2883695907aeaa1c2370ae1f21d6","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/shihao_iccv2021/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/post/shihao_iccv2021/","section":"post","summary":" ","tags":null,"title":"Our paper \"EventHPE: Event-based 3-D Human Pose Estimation\" is accepted by ICCV 2021","type":"post"},{"authors":["Taivanbat Badamdorj","Mrigank Rochan","Yang Wang","Li Cheng"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"ad64b0b5097571419e42715ce7d8ed03","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/tk_iccv2021/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/post/tk_iccv2021/","section":"post","summary":" ","tags":null,"title":"Our paper \"Joint Visual and Audio Learning for Video Highlight Detection\" is accepted by ICCV 2021","type":"post"},{"authors":["Wei Ji","Jingjing Li","Shuang Yu","Miao Zhang","Yongri Piao","Shunyu Yao","Qi Bi","Kai Ma","Yefeng Zheng","Huchuan Lu","Li Cheng"],"categories":null,"content":"","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"2b4b5e68e32bf11f914e812d694aa087","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/wei_cvpr_2021a/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/post/wei_cvpr_2021a/","section":"post","summary":"This paper systematically addresses the depth-related side effects via the designed calibration strategy towards boosting saliency detection accuracy.","tags":null,"title":"Our paper \"Calibrated RGB-D Salient Object Detection\" is accepted by IEEE Conference on Computer Vision and Pattern Recognition 2021","type":"post"},{"authors":["Wei Ji","Shuang Yu","Junde Wu","Kai Ma","Cheng Bian","Qi Bi","Jingjing Li","Hanruo Liu","Li Cheng","Yefeng Zheng"],"categories":null,"content":"","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"9ed4a9a2a9cb45c4405cefc6824ae43e","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/wei_cvpr_2021b/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/post/wei_cvpr_2021b/","section":"post","summary":"This paper proposes a principled research investigation on exploiting the rich agreement information among multiple raters for improving the calibrated performance.","tags":null,"title":"Our paper \"Learning Calibrated Medical Image Segmentation via Multi-rater Agreement Modeling\" is accepted by IEEE Conference on Computer Vision and Pattern Recognition 2021 (Best Paper Candidate)","type":"post"},{"authors":["Wei Ji","Jingjing Li","Shuang Yu","Miao Zhang","Yongri Piao","Shunyu Yao","Qi Bi","Kai Ma","Yefeng Zheng","Huchuan Lu","Li Cheng"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"806baf62fd136911f272556d2dedd828","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wei_cvpr21_dcf/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/wei_cvpr21_dcf/","section":"publication","summary":"This paper systematically addresses the depth-related side effects via the designed calibration strategy towards boosting saliency detection accuracy.","tags":["CVPR"],"title":"Calibrated RGB-D Salient Object Detection","type":"publication"},{"authors":["Wei Ji","Shuang Yu","Junde Wu","Kai Ma","Cheng Bian","Qi Bi","Jingjing Li","Hanruo Liu","Li Cheng","Yefneg Zhang"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"3d8cbe1eae0f7df3f68f82c3ee4a42df","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wei_cvpr21_mrnet/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/wei_cvpr21_mrnet/","section":"publication","summary":"This paper proposes a principled research investigation on exploiting the rich agreement information among multiple raters for improving the calibrated performance.","tags":["CVPR"],"title":"Learning Calibrated Medical Image Segmentation via Multi-rater Agreement Modeling","type":"publication"},{"authors":["Seyed Mojtaba Marvasti-Zadeh","Li Cheng","Hossein Ghanei-Yakhdan","Shohreh Kasaei"],"categories":null,"content":"","date":1606435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606435200,"objectID":"586137e0de033f7721f35b40d7b78d12","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/mojtaba_tits2020/","publishdate":"2020-11-27T00:00:00Z","relpermalink":"/post/mojtaba_tits2020/","section":"post","summary":"","tags":null,"title":"Our paper \"Deep Learning for Visual Tracking: A Comprehensive Survey\" is accepted by IEEE Transactions on Intelligent Transportation Systems (T-ITS)","type":"post"},{"authors":["Chuan Guo","Xinxin Zuo","Sen Wang","Shihao Zou","Qingyao Sun","Annan Deng","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"6516898824f6ccdac9759c900ecfc801","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/chuan_action2motion_mm/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/chuan_action2motion_mm/","section":"publication","summary":"A temporal VAE archtecture model equipped with Lie Algebra representation for action-conditioned 3D human motion generation.","tags":["MM","Motion Generation","Tag3"],"title":"Action2Motion: Conditioned Generation of 3D Human Motions","type":"publication"},{"authors":["Chuan Guo","Xinxin Zuo","Sen Wang","Shihao Zou","Qingyao Sun","Annan Deng","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1595635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595635200,"objectID":"3ef6bb36c36513693a3b5b554d09d17c","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/chuan_acmmm2020/","publishdate":"2020-07-25T00:00:00Z","relpermalink":"/post/chuan_acmmm2020/","section":"post","summary":" ","tags":null,"title":"Our paper \"Action2Motion: Conditioned Generation of 3-D Human Motions\" is accepted by ACM Multimedia 2020","type":"post"},{"authors":["Shihao Zou","Xinxin Zuo","Sen Wang","Li Cheng"],"categories":null,"content":"","date":1593820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593820800,"objectID":"60fd10ee83d313d543686c13fcca26d3","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/zou_eccv2020/","publishdate":"2020-07-04T00:00:00Z","relpermalink":"/post/zou_eccv2020/","section":"post","summary":" ","tags":null,"title":"Our paper \"3D Human Shape Reconstruction from a Polarization Image\" is accepted by ECCV 2020","type":"post"},{"authors":["Shihao Zou","Xinxin Zuo","Yiming Qian","Sen Wang","Chi Xu","Minglun Gong","Li Cheng"],"categories":null,"content":"\rClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.\r\r\r\rClick the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.\r\r\rSupplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --\r","date":1593648000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593648000,"objectID":"8552f1b3ed1ccd24d1d946c2f940bfd1","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shihaozou_polarization_2020/","publishdate":"2020-07-02T00:00:00Z","relpermalink":"/publication/shihaozou_polarization_2020/","section":"publication","summary":"This paper tackles a new problem of estimating 3D body shape of clothed humans from single polarized 2D images, i.e. a polarization image.","tags":["ECCV"],"title":"3D Human Shape Reconstruction from a Polarization Image","type":"publication"},{"authors":["Mahdiar Nekoui","Fidel Omar Tito Cruz","Li Cheng"],"categories":null,"content":"","date":1592524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592524800,"objectID":"670a8a94f576b2ff0f0f4efb5dc9b453","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/mahdiar_falcons_2020/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/mahdiar_falcons_2020/","section":"publication","summary":"Isn't it about time to help judges with the challenging task of evaluating athletes' performances in sports with extreme poses? To tackle this problem and inspired by human judges' grading schema, we propose a virtual refereeing network to evaluate the execution of a diving performance. This assessment would be based on visual clues as well as the body joints sequence of the action video. In order to cover the unusual body contortions in such scenarios, we present ExPose: annotated dataset of Extreme Poses. We further introduce a simple yet effective module to assess the difficulty of the performance based on the extracted joints sequence. Finally, the overall score of the performance would be reported as the multiplication of the execution and difficulty scores. The results demonstrate our proposed lightweight network not only achieves state-of-the-art results compared to previous studies in diving but also shows acceptable generalization to other contortive sports.","tags":["CVPR","CVPRW"],"title":"FALCONS: FAst Learner-grader for CONtorted poses in Sports","type":"publication"},{"authors":["Xinxin Zuo","Sen Wang","Jiangbin Zheng","Weiwei Yu","Minglun Gong","Ruigang Yang","Li Cheng"],"categories":null,"content":"","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591747200,"objectID":"dfd06bfcf83548aba78abc0b07d14732","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xinxin_tmm_2020/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/publication/xinxin_tmm_2020/","section":"publication","summary":"A novel approach to reconstruct 3D human body shapes based on a sparse set of RGBD frames using a single RGBD camera","tags":["TMM"],"title":"SparseFusion: Dynamic Human Avatar Modeling from Sparse RGBD Images","type":"publication"},{"authors":["Ji Yang","Youdong Ma","Li Cheng"],"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"b23808b5539c562cda8fb8b481fff4c2","permalink":"https://vision-and-learning-lab-ualberta.github.io/project/jiyang_multitask_2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/project/jiyang_multitask_2020/","section":"project","summary":"A multitask learning system for 3D pose esitimation and future motion prediction from video.","tags":["Deep Learning","Pose Estimation","Motion Prediction"],"title":"3D Pose Estimation and Future Motion Prediction from 2D Images","type":"project"},{"authors":["A. Banerjeea","S. Wu","Li Cheng","S. Aw"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"11891018b89e96395217352e7a1cc61b","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/ban-et-al-jo-ve-20/","publishdate":"2020-06-22T11:05:44.337563Z","relpermalink":"/publication/ban-et-al-jo-ve-20/","section":"publication","summary":"","tags":null,"title":"Fully automated leg movement tracking in freely moving insects using Feature Learning Leg Segmentation and Tracking (FLLIT)","type":"publication"},{"authors":["P. Porwal","S. Pachade","M. Kokare","et al."],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"3902ddd851b531e5742c6791d35f1d10","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/lyu-et-al-mia-20/","publishdate":"2020-06-22T11:05:44.254436Z","relpermalink":"/publication/lyu-et-al-mia-20/","section":"publication","summary":"","tags":null,"title":"IDRiD: Diabetic Retinopathy - Segmentation and Grading Challenge","type":"publication"},{"authors":["He Zhao","Huiqi Li","Li Cheng"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"46ab12980ee65e32440e802237ffc6f9","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-pr-20/","publishdate":"2020-06-22T11:05:44.255024Z","relpermalink":"/publication/zha-et-al-pr-20/","section":"publication","summary":"","tags":null,"title":"Improving retinal vessel segmentation with joint local loss by matting","type":"publication"},{"authors":["D. Teng","X. Zhang","Li Cheng","Delin Chu"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"4b8e629432ff5b1aec2edac4efebc3fb","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/ten-et-al-tbd-20/","publishdate":"2020-06-22T11:05:44.253734Z","relpermalink":"/publication/ten-et-al-tbd-20/","section":"publication","summary":"","tags":null,"title":"Least Squares Approximation via Sparse Subsampled Randomized Hadamard Transform","type":"publication"},{"authors":["Shuang Wu","Zhenguang Liu","Shuyuan Jin","Qi Liu","Shijian Lu","Roger Zimmermann","Li Cheng"],"categories":null,"content":"\rClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.\r\r\r\rClick the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.\r\r\rSupplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --\r","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"2a84508ad3130d7bcd6062250fb085b5","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shuangwu_hmr_2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/shuangwu_hmr_2019/","section":"publication","summary":"A hierarchical recurrent network structure is developed to simultaneously encodes local contexts of individual frames and global contexts of the sequence.","tags":["CVPR"],"title":"Towards Natural and Accurate Future Motion Prediction of Humans and Animals","type":"publication"},{"authors":["Shuang Wu","Zhenguang Liu","Shuyuan Jin","Qi Liu","Shijian Lu","Roger Zimmermann","Li Cheng"],"categories":null,"content":"","date":1556323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556323200,"objectID":"545a19981a73ac7fa09f37b55cef1da4","permalink":"https://vision-and-learning-lab-ualberta.github.io/project/shuangwu_hmr_2019/","publishdate":"2019-04-27T00:00:00Z","relpermalink":"/project/shuangwu_hmr_2019/","section":"project","summary":"Towards natural and accurate future motion prediction of humans and animals.","tags":["Deep Learning","Motion Prediction"],"title":"Hierarchical Motion Recurrent Network for Future Motion Prediction of Humans and Animals","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic \rAcademic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click \rPDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? \rAsk\n\rDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://vision-and-learning-lab-ualberta.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["S. Wu","K. Tan","L. Govindarajan","J. Stewart","H. Zhu","L. Gu","M. Katarya","B. Wong","E. Tan","D. Li","A. Chang","C. Libedinsky","L. Cheng","S. Aw"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3e32d70ceb0a4b4f0c1a3009e20d24d1","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wu-et-al-plos-bio-19/","publishdate":"2020-06-22T11:05:44.25635Z","relpermalink":"/publication/wu-et-al-plos-bio-19/","section":"publication","summary":"","tags":null,"title":"Fully automated leg tracking of Drosophila neurodegeneration models reveals distinct conserved movement signatures","type":"publication"},{"authors":["Xiaowei Zhang","Xudong Shi","Yu Sun","Li Cheng"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"571d49abeef592e0e8a9fdb4ef70b42d","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-tpami-19/","publishdate":"2020-06-22T11:05:44.25559Z","relpermalink":"/publication/zha-et-al-tpami-19/","section":"publication","summary":"","tags":null,"title":"Multivariate Regression with Gross Errors on Manifold-valued Data","type":"publication"},{"authors":["Qi Liu","Yue Zhang","Ye Yuan","Zhenguang Liu","Li Cheng","Roger Zimmermann"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"11a39798d5ff2d76dbfe7ac12f6424dc","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/liu-et-al-aaai-18/","publishdate":"2020-06-22T11:05:44.298671Z","relpermalink":"/publication/liu-et-al-aaai-18/","section":"publication","summary":"","tags":null,"title":"Multi-modal Multi-task Learning for Automatic Dietary Assessment","type":"publication"},{"authors":["He Zhao","Huiqi Li","Sebastian Maurer-Stroh","Yuhong Guo","Qiuju Deng","Li Cheng"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"7e0d96354751b82102a25c1e1eab5802","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-tmi-18/","publishdate":"2020-06-22T11:05:44.257173Z","relpermalink":"/publication/zha-et-al-tmi-18/","section":"publication","summary":"","tags":null,"title":"Supervised Segmentation of Un-annotated Retinal Fundus Images by Synthesis","type":"publication"},{"authors":["He Zhao","Huiqi Li","Sebastian Maurer-Stroh","Li Cheng"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"03b1eecc59ba0893dbf01e6e450d731e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-med-ia-18/","publishdate":"2020-06-22T11:05:44.257789Z","relpermalink":"/publication/zha-et-al-med-ia-18/","section":"publication","summary":"","tags":null,"title":"Synthesizing Retinal and Neuronal Images with Generative Adversarial Nets","type":"publication"},{"authors":["X. Zhang","L. Cheng","B. Li","Hai-Miao Hu"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d82f20796ab170cb4b9ce5f0480a5f6e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-tip-18/","publishdate":"2020-06-22T11:05:44.258366Z","relpermalink":"/publication/zha-et-al-tip-18/","section":"publication","summary":"","tags":null,"title":"Too Far to See? Not Really! Pedestrian Detection with Scale-aware Localization Policy","type":"publication"},{"authors":["J. De","X. Zhang","F. Lin","L. Cheng"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"552e26bbae97598652181a7c05bd1012","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/de-et-al-tpami-18/","publishdate":"2020-06-22T11:05:44.258979Z","relpermalink":"/publication/de-et-al-tpami-18/","section":"publication","summary":"","tags":null,"title":"Transduction on Directed Graphs via Absorbing Random Walks","type":"publication"},{"authors":["Z. Liu","L. Zhang","Q. Liu","Y. Yin","L. Cheng","R. Zimmermann"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"fc8480a63324950d61b154c2398d542e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/liu-et-al-tmm-17/","publishdate":"2020-06-22T11:05:44.265416Z","relpermalink":"/publication/liu-et-al-tmm-17/","section":"publication","summary":"","tags":null,"title":"Fusion of Magnetic and Vision sensors for indoor localization: Infrastructure-free and More Effective","type":"publication"},{"authors":["C. Xu","L. Govindarajan","L. Cheng"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"8e22d2ea911eb00a3732823a654b3603","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xu-gov-che-pr-17/","publishdate":"2020-06-22T11:05:44.259655Z","relpermalink":"/publication/xu-gov-che-pr-17/","section":"publication","summary":"","tags":null,"title":"Hand Action Detection from Ego-centric Depth Sequences","type":"publication"},{"authors":["Ch. Xu","L. Govindarajan","Y. Zhang","L. Cheng"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"2e03cee8ed5a8d5f0ae5a90cee510234","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xu-et-al-ijcv-17/","publishdate":"2020-06-22T11:05:44.261455Z","relpermalink":"/publication/xu-et-al-ijcv-17/","section":"publication","summary":"","tags":null,"title":"Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking, and Action Recognition on Lie Groups","type":"publication"},{"authors":["Zhenguang Liu","L. Cheng","Anan Liu","Luming Zhang","Xiangnan He","Roger Zimmermann"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"ceff61c53181010dba85e7e5303f8f19","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/liu-et-al-mm-17/","publishdate":"2020-06-22T11:05:44.299757Z","relpermalink":"/publication/liu-et-al-mm-17/","section":"publication","summary":"","tags":null,"title":"Multiview and Multimodal Pervasive Indoor Localization","type":"publication"},{"authors":["Ch. Xu","L. Zhang","L. Cheng","R. Koch"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"b7b413d24d6c8921f7d465dc48701337","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xu-et-al-tpami-17/","publishdate":"2020-06-22T11:05:44.262471Z","relpermalink":"/publication/xu-et-al-tpami-17/","section":"publication","summary":"","tags":null,"title":"Pose Estimation from Line Correspondences: A Complete Analysis and A Series of Solutions","type":"publication"},{"authors":["A. Cliffe","D. Doupe","H. Sung","L. Hwee","L. Cheng","K. Ong","W. Yu"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"e968ecb9ee98294495a15e5b93d8198f","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/cli-et-al-nc-17/","publishdate":"2020-06-22T11:05:44.26048Z","relpermalink":"/publication/cli-et-al-nc-17/","section":"publication","summary":"","tags":null,"title":"Quantitative 3D analysis of complex single border cell behaviors in coordinated collective cell migration","type":"publication"},{"authors":["H. Tie","B. Chen","X. Sun","L. Cheng","L. Lu"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"8ac9f8f0a8d1cbc0496e189abf202c46","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/tie-et-al-jo-ve-17/","publishdate":"2020-06-22T11:05:44.338633Z","relpermalink":"/publication/tie-et-al-jo-ve-17/","section":"publication","summary":"","tags":null,"title":"Quantitative localization of a Golgi protein by imaging its fluorescence center of mass","type":"publication"},{"authors":["G. Lin","X. Zhang","H. Zhao","H. Li","L. Cheng"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f557646c0300cae4e2ea2510bd524387","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gu-et-al-tmi-17/","publishdate":"2020-06-22T11:05:44.264337Z","relpermalink":"/publication/gu-et-al-tmi-17/","section":"publication","summary":"","tags":null,"title":"Segment 2D and 3D Filaments by Learning Structured and Contextual Features","type":"publication"},{"authors":["Li Cheng"],"categories":null,"content":"Joining us\u0026hellip; The Vision \u0026amp; Learning Lab at the ECE Dept. University of Alberta, is inviting outstanding Postdoc candidates to join us.\nWe are setting up a new lab at the ECE Dept., University of Alberta, focusing on exciting research topics in computer vision and machine learning. We are looking for exceptional Postdocs to join us.\nFurther information about the PI, Dr. Li Cheng, can be found below, or via https://www.ece.ualberta.ca/~lcheng5/ or by direct inquiries to lcheng5@ualberta.ca.\nPotential candidates are requested to email their CVs (in PDF) to Li Cheng (lcheng5@ualberta.ca).\nAbout the PI\u0026hellip; Li CHENG is an associate professor with the ECE Dept., University of Alberta, Canada. His research expertise is mainly in computer vision and machine learning, with application focus in both visual behavior analysis and biomedical image analysis. His research work has resulted in over 90 referred papers including those published at journals such as IEEE Trans. Pattern Analysis and Machine Intelligence, International Journal of Computer Vision, as well as conferences such as ICML, NIPS, ICCV, CVPR, MICCAI, and AAAI. He is a senior member of IEEE.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"7cc43f76fb802bdd6f3dba71478b786e","permalink":"https://vision-and-learning-lab-ualberta.github.io/archive/opening_postdoc/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/archive/opening_postdoc/","section":"archive","summary":" ","tags":["Recruiting"],"title":"","type":"archive"},{"authors":["J. De","L. Cheng","X. Zhang","F. Lin","H. Li","K. Ong","W. Yu","Y. Yu","S. Ahmed"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"0a270a7876518a81ca76879405493622","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/de-et-al-tmi-16/","publishdate":"2020-06-22T11:05:44.271519Z","relpermalink":"/publication/de-et-al-tmi-16/","section":"publication","summary":"","tags":null,"title":"A Graph-theoretical Approach for Tracing Filamentary Structures in Neuronal and Retinal Images","type":"publication"},{"authors":["H. Tie","D. Mahajan","B. Chen","L. Cheng","A. VanDongen","L. Lu"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"24fc22998ffe65d9b3b0d943fd6a36a3","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/tie-et-al-m-bo-c-16/","publishdate":"2020-06-22T11:05:44.269271Z","relpermalink":"/publication/tie-et-al-m-bo-c-16/","section":"publication","summary":"","tags":null,"title":"A novel imaging method for quantitative Golgi localization reveals differential intra-Golgi trafficking of secretory cargos","type":"publication"},{"authors":["Y. Zhang","L. Cheng","J. Wu","J. Cai","Mi. Do","J. Lu"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"447d1554d98dc2e50c0ea91257e5b42e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-tip-16/","publishdate":"2020-06-22T11:05:44.266533Z","relpermalink":"/publication/zha-et-al-tip-16/","section":"publication","summary":"","tags":null,"title":"Action Recognition in Still Images with Minimum Annotation Efforts","type":"publication"},{"authors":["C. Xu","A. Nanjappa","X. Zhang","L. Cheng"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"7a01a6f825531ffb0b3cc1289a5f8510","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xu-et-al-ijcv-16/","publishdate":"2020-06-22T11:05:44.270387Z","relpermalink":"/publication/xu-et-al-ijcv-16/","section":"publication","summary":"","tags":null,"title":"Estimate Hand Poses Efficiently from Single Depth Images","type":"publication"},{"authors":["X. Zhang","L. Cheng","D. Chu","L. Liao","M. Ng","R. Tan"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"f549739d92a2c0f29ffa8a5c02760bbf","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-sisc-16/","publishdate":"2020-06-22T11:05:44.263212Z","relpermalink":"/publication/zha-et-al-sisc-16/","section":"publication","summary":"","tags":null,"title":"Incremental Regularized Least Squares for Dimensionality Reduction of Large-Scale Data","type":"publication"},{"authors":["K. Ong","J. De","L. CHENG","S. AHMED","W. YU"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"5f7318a8c58c42861a8ef0004fa861c5","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/ong-et-al-cyto-16/","publishdate":"2020-06-22T11:05:44.26779Z","relpermalink":"/publication/ong-et-al-cyto-16/","section":"publication","summary":"","tags":null,"title":"NeuronCyto II: An Automatic and Quantitative Solution for Crossover Neural Cells in High Throughput Screening","type":"publication"},{"authors":["Li Liu","Li Cheng","Ye Liu","Yongpo Jia","David S. Rosenblum"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"ab46e053f97ae942da9a941dfdb98a8f","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/liu-et-al-aaai-16/","publishdate":"2020-06-22T11:05:44.300848Z","relpermalink":"/publication/liu-et-al-aaai-16/","section":"publication","summary":"","tags":null,"title":"Recognizing Complex Activities by a Probabilistic Interval-based Model","type":"publication"},{"authors":["Yiming Qian","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"df17b013bb6c45e7dfffefa7193a2ff4","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/qia-gon-che-cai-15/","publishdate":"2020-06-22T11:05:44.305156Z","relpermalink":"/publication/qia-gon-che-cai-15/","section":"publication","summary":"","tags":null,"title":"An Efficient Self-Tuning Multiclass Classification Approach","type":"publication"},{"authors":["C. Yap","E. Kalaw","M. Singh","K. Chong","D. Giron","C. Huang","L. Cheng","Y. Law","H. Lee"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"b453890df6f78ee064da10794f2f0220","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/yap-et-al-jpi-15/","publishdate":"2020-06-22T11:05:44.273834Z","relpermalink":"/publication/yap-et-al-jpi-15/","section":"publication","summary":"","tags":null,"title":"Automated Image Based Prominent Nucleoli Detection","type":"publication"},{"authors":["Ashwin Nanjappa","Chi Xu","Li Cheng"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"e33357c5bfd594b8a0f4651b45ace217","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/nan-chi-che-eurographics-15/","publishdate":"2020-06-22T11:05:44.304079Z","relpermalink":"/publication/nan-chi-che-eurographics-15/","section":"publication","summary":"","tags":null,"title":"GHand: A GPU algorithm for realtime hand pose estimation using depth camera","type":"publication"},{"authors":["M. Gong","Y. Qian","L. Cheng"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"c7562d1345aadb3f03d8e8b4156c58a1","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gon-qia-che-tip-15/","publishdate":"2020-06-22T11:05:44.272652Z","relpermalink":"/publication/gon-qia-che-tip-15/","section":"publication","summary":"","tags":null,"title":"Integrated Foreground Segmentation and Boundary Matting for Live Videos","type":"publication"},{"authors":["Lin Gu","Li Cheng"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"e01d1698b96d566a0e23b02e9636c30b","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gu-che-iccv-15/","publishdate":"2020-06-22T11:05:44.303057Z","relpermalink":"/publication/gu-che-iccv-15/","section":"publication","summary":"","tags":null,"title":"Learning to Boost Filamentary Structure Segmentation","type":"publication"},{"authors":["Xiaowei Zhang","Li Cheng","Tingshao Zhu"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"7a67c3cd24cd4c29059055e207906790","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-che-zhu-acml-15/","publishdate":"2020-06-22T11:05:44.302008Z","relpermalink":"/publication/zha-che-zhu-acml-15/","section":"publication","summary":"","tags":null,"title":"Robust Multivariate Regression with Grossly Corrupted Observations and Its Application to Personality Prediction","type":"publication"},{"authors":["Meiguang Jin","Lakshmi Govindarajan","L. Cheng"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"7b27085996bdc122fd10d3fd44e14b38","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/jin-et-al-isbi-14/","publishdate":"2020-06-22T11:05:44.307353Z","relpermalink":"/publication/jin-et-al-isbi-14/","section":"publication","summary":"","tags":null,"title":"A Random-Forest Random Field Approach for Cellular Image Segmentation","type":"publication"},{"authors":["Jia Zhang","Huiqi Li","Qing Nie","Li Cheng"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"5fdbe21def8bcaef7d9bdbe6a0cdc41c","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-cmig-14/","publishdate":"2020-06-22T11:05:44.279667Z","relpermalink":"/publication/zha-et-al-cmig-14/","section":"publication","summary":"","tags":null,"title":"A retinal vessel boundary tracking method based on Bayesian theory and multi-scale line detection","type":"publication"},{"authors":["K. Yong","T. Gong","M. Nongpiur","A. How","H. Lee","L. Cheng","S. Perera","T. Aung"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"71aea105d043b9485bc54407c079764e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/yon-et-al-optho-14/","publishdate":"2020-06-22T11:05:44.278501Z","relpermalink":"/publication/yon-et-al-optho-14/","section":"publication","summary":"","tags":null,"title":"Myopia in Asian Subjects with Primary Angle Closure: Implications for Glaucoma Trends in East Asia","type":"publication"},{"authors":["T. Thi","L. Wang","J. Zhang","S. Maurer-Stroh","L. Cheng"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"bcf2eb55b524c5d3822583b2a321a43e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-et-al-bmcbioinfo-14/","publishdate":"2020-06-22T11:05:44.277425Z","relpermalink":"/publication/thi-et-al-bmcbioinfo-14/","section":"publication","summary":"","tags":null,"title":"Recognizing Flu-like Symptoms from Videos","type":"publication"},{"authors":["Li Cheng","Sinno Jialin Pan"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"bb049cffd713d732134973aff98876de","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-pan-tnnls-14/","publishdate":"2020-06-22T11:05:44.274967Z","relpermalink":"/publication/che-pan-tnnls-14/","section":"publication","summary":"","tags":null,"title":"Semi-supervised Domain Adaptation on Manifolds","type":"publication"},{"authors":["Li Cheng","Jaydeep De","Xiaowei Zhang","Feng Lin","Huiqi Li"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"462de3588e5790ed294c8275fc309fe9","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-et-al-miccai-14/","publishdate":"2020-06-22T11:05:44.306289Z","relpermalink":"/publication/che-et-al-miccai-14/","section":"publication","summary":"","tags":null,"title":"Tracing Retinal Blood Vessels by Matrix-Forest Theorem of Directed Graphs","type":"publication"},{"authors":["Jaydeep De","Huiqi Li","Li Cheng"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"de5fb8c09ddbc1b43e8fb1ddbc590a6a","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/de-li-che-bmcbioinfo-14/","publishdate":"2020-06-22T11:05:44.276096Z","relpermalink":"/publication/de-li-che-bmcbioinfo-14/","section":"publication","summary":"","tags":null,"title":"Tracing retinal vessel trees by transductive inference","type":"publication"},{"authors":["C. Guzman","T. Gong","M. Nongpiur","S. Perera","A. How","H. Lee","L. Cheng","M. He","M. Baskaran","T. Aung"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"7d00e522f70559e8949dd055afe3e4c0","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/guz-et-al-iovs-13/","publishdate":"2020-06-22T11:05:44.284382Z","relpermalink":"/publication/guz-et-al-iovs-13/","section":"publication","summary":"","tags":null,"title":"Anterior segment optical coherence tomography parameters in subtypes of primary angle closure","type":"publication"},{"authors":["Jaydeep De","Tengfei Ma","Huiqi Li","M. Dash","Li Cheng"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"7f100743faa4ac8b6efe3ecaec79e806","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/jay-et-al-scia-13/","publishdate":"2020-06-22T11:05:44.339665Z","relpermalink":"/publication/jay-et-al-scia-13/","section":"publication","summary":"","tags":null,"title":"Automated Tracing of Retinal Blood Vessels Using Graphical Models","type":"publication"},{"authors":["Matti Pietikainen","Matthew Turk","Liang Wang","Guoyin Zhao","Li Cheng"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"7125bee57911f3ca31ce5ecb430fab1c","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/pie-et-al-ivc-13/","publishdate":"2020-06-22T11:05:44.28083Z","relpermalink":"/publication/pie-et-al-ivc-13/","section":"publication","summary":"","tags":null,"title":"Editorial of the Special issue: Machine Learning in Motion Analysis: New Advances","type":"publication"},{"authors":["Chi Xu","Li Cheng"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"0fbbc47a3cf8250ced941303eff64a19","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xu-che-iccv-13/","publishdate":"2020-06-22T11:05:44.308441Z","relpermalink":"/publication/xu-che-iccv-13/","section":"publication","summary":"","tags":null,"title":"Efficient Hand Pose Estimation from a Single Depth Image","type":"publication"},{"authors":["Shaojun Wang","Shaomin Wang","Li Cheng","Russell Greiner","Dale Schuurman"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"d697c66b9565d3c468a85ea23042dd0f","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wan-et-al-ci-13/","publishdate":"2020-06-22T11:05:44.281846Z","relpermalink":"/publication/wan-et-al-ci-13/","section":"publication","summary":"","tags":null,"title":"Exploiting Syntactic, Semantic, and Lexical Regularities in Language Modeling via Directed Markov Random Fields","type":"publication"},{"authors":["Tianxia Gong","Li Cheng Nengli Lim","Hwee Kuan Lee","Bolan Su","Shimiao Li","Chew Lim Tan","Boon Chuan Pang","C. C. Tchoyoson Lim","Cheng Kiang Lee"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"ad1e5df72c9837317233d4fa8cca0548","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gon-et-al-ictai-13/","publishdate":"2020-06-22T11:05:44.311341Z","relpermalink":"/publication/gon-et-al-ictai-13/","section":"publication","summary":"","tags":null,"title":"Finding Distinctive Shape Features for Hematoma Classification in Brain CT Images","type":"publication"},{"authors":["Li Cheng"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"06f5ab303cbea1e398ad4026b7212c6b","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-icml-13/","publishdate":"2020-06-22T11:05:44.310327Z","relpermalink":"/publication/che-icml-13/","section":"publication","summary":"","tags":null,"title":"Riemannian Similarity Learning","type":"publication"},{"authors":["M. Nongpiur","T. Gong","H. Lee","S. Perera","L. Cheng","L. Foo","M. He","D. Friedman","T. Aung"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"6a7525eea9d939bc9295e0d068068181","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/non-et-al-optho-13/","publishdate":"2020-06-22T11:05:44.282988Z","relpermalink":"/publication/non-et-al-optho-13/","section":"publication","summary":"","tags":null,"title":"Subgrouping of Primary Angle-Closure Suspects Based on Anterior Segment Optical Coherence Tomography Parameters","type":"publication"},{"authors":["Li Cheng","Ning Ye","Weimiao Yu","Andre Cheah"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"b46f4814956cf1f3c30280b52c8c5216","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-et-al-springerbkchapt-12/","publishdate":"2020-06-22T11:05:44.340699Z","relpermalink":"/publication/che-et-al-springerbkchapt-12/","section":"publication","summary":"","tags":null,"title":"A Bag-of-Words Model for Cellular Image Segmentation","type":"publication"},{"authors":["Tuan Hue Thi","Li Cheng","Jian Zhang","Li Wang","Shinichi Satoh"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"34c34ad3697beaee3bf378bcf9c82a72","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-et-al-cviu-12/","publishdate":"2020-06-22T11:05:44.286217Z","relpermalink":"/publication/thi-et-al-cviu-12/","section":"publication","summary":"","tags":null,"title":"Integrating Local Action Elements for Action Analysis","type":"publication"},{"authors":["Tuan Hue Thi","Li Cheng","Jian Zhang","Li Wang","Shinichi Satoh"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"616627fb9317c85b54c3446c86ce7b27","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-et-al-ivc-12/","publishdate":"2020-06-22T11:05:44.287368Z","relpermalink":"/publication/thi-et-al-ivc-12/","section":"publication","summary":"","tags":null,"title":"Structured learning of local features for human action classification and localization","type":"publication"},{"authors":["Li Cheng","Ning Ye","Weimiao Yu","Andre Cheah"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"b567d835880598d4ef7bcc9d56de5412","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-et-al-miccai-11/","publishdate":"2020-06-22T11:05:44.312474Z","relpermalink":"/publication/che-et-al-miccai-11/","section":"publication","summary":"","tags":null,"title":"Discriminative Cellular Segmentation for Microscopic Images","type":"publication"},{"authors":["Qinfeng Shi","Li Wang","Li Cheng","Alex Smola"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"f0693617850063796af08b29b72387a3","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shi-et-al-ijcv-11/","publishdate":"2020-06-22T11:05:44.290469Z","relpermalink":"/publication/shi-et-al-ijcv-11/","section":"publication","summary":"","tags":null,"title":"Discriminative Human Action Segmentation and Recognition using SMMs","type":"publication"},{"authors":["Li Wang","Li Cheng","Li Wang"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"2ca7e891bbea008d55a2d587fde815a9","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wan-che-wan-tip-11/","publishdate":"2020-06-22T11:05:44.289428Z","relpermalink":"/publication/wan-che-wan-tip-11/","section":"publication","summary":"","tags":null,"title":"Elastic Sequence Correlation for Human Action Analysis","type":"publication"},{"authors":["Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"80601a8f9170c6d1e558856a17f04670","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gon-che-cvpr-11/","publishdate":"2020-06-22T11:05:44.313469Z","relpermalink":"/publication/gon-che-cvpr-11/","section":"publication","summary":"","tags":null,"title":"Foreground Segmentation of Live Videos using Locally Competing 1SVMs","type":"publication"},{"authors":["Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"f677554c8791f9823dab099e516062b4","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gon-che-icip-11/","publishdate":"2020-06-22T11:05:44.314434Z","relpermalink":"/publication/gon-che-icip-11/","section":"publication","summary":"","tags":null,"title":"Incorporating Estimated Motion in Real-time Background Subtraction","type":"publication"},{"authors":["Li Cheng","Minglun Gong","Dale Schuurmans","Terry Caelli"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"13c92323c1b0a800b9e2d670a555944e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-et-al-tip-11/","publishdate":"2020-06-22T11:05:44.288413Z","relpermalink":"/publication/che-et-al-tip-11/","section":"publication","summary":"","tags":null,"title":"Real-time Discriminative Background Subtraction","type":"publication"},{"authors":["Ke Jia","Li Cheng","Nianjun Liu","Li Wang"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"7b4c11ec6e7327a4a99994e5bfcc055e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/jia-che-liu-wan-icpr-10/","publishdate":"2020-06-22T11:05:44.316613Z","relpermalink":"/publication/jia-che-liu-wan-icpr-10/","section":"publication","summary":"","tags":null,"title":"Efficient Learning to Label Images","type":"publication"},{"authors":["Tuan Hue Thi","Li Cheng","Jian Zhang","Li Wang","Shinichi Satoh"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"97a1772938234256609fc1bbfd645bf7","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-che-zha-wan-sat-avss-10/","publishdate":"2020-06-22T11:05:44.320234Z","relpermalink":"/publication/thi-che-zha-wan-sat-avss-10/","section":"publication","summary":"","tags":null,"title":"Human Action Recognition and Localization in Video using Structured Learning of Local Space-Time Features","type":"publication"},{"authors":["Li Wang","Li Cheng","Tuan Hue Thi","Jian Zhang"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"25c901d51d5ca04fa5bff8f5303fb64a","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wan-che-thi-zha-dicta-10/","publishdate":"2020-06-22T11:05:44.315447Z","relpermalink":"/publication/wan-che-thi-zha-dicta-10/","section":"publication","summary":"","tags":null,"title":"Human Action Recognition from Boosted Pose Estimation","type":"publication"},{"authors":["Tuan Hue Thi","Li Cheng","Jian Zhang","Li Wang"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"3c6a5f79d611d44cb85d7650ac426e9e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-che-zha-wan-icpr-10/","publishdate":"2020-06-22T11:05:44.319191Z","relpermalink":"/publication/thi-che-zha-wan-icpr-10/","section":"publication","summary":"","tags":null,"title":"Implicit Motion-Shape Model: A generic approach for action matching","type":"publication"},{"authors":["Tiberio Caetano","Julian McAuley","Li Cheng","Quoc Le","Alex Smola"],"categories":null,"content":"","date":1243814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1243814400,"objectID":"8b54e7867a12a56acd783704b3ba98f4","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/cae-et-al-tpami-09/","publishdate":"2020-06-22T11:05:44.291744Z","relpermalink":"/publication/cae-et-al-tpami-09/","section":"publication","summary":"","tags":null,"title":"Learning Graph Matching","type":"publication"},{"authors":["Tuan Hue Thi","Sijun Lu","Jian Zhang","Li Cheng","Li Wang"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"5060980fb27e086c56866f23f2559129","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-et-al-avss-09/","publishdate":"2020-06-22T11:05:44.323362Z","relpermalink":"/publication/thi-et-al-avss-09/","section":"publication","summary":"","tags":null,"title":"Human Body Articulation for Action Recognition in Video Sequences","type":"publication"},{"authors":["Yuxi Li","Li Cheng","Dale Schuurmans"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"a13b6403abb83d196f57d01e5fa24675","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/li-che-sch-cife-09/","publishdate":"2020-06-22T11:05:44.342988Z","relpermalink":"/publication/li-che-sch-cife-09/","section":"publication","summary":"","tags":null,"title":"Inference of the Structural Credit Risk Model using MLE","type":"publication"},{"authors":["Baochun Bai","Li Cheng","Cheng Lei","Pierre Boulanger","Janelle Harms"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"faaa14d7121583ac90bc32d8f47f08cb","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/bao-et-al-pcs-09/","publishdate":"2020-06-22T11:05:44.322355Z","relpermalink":"/publication/bao-et-al-pcs-09/","section":"publication","summary":"","tags":null,"title":"Learning-based multiview video coding","type":"publication"},{"authors":["Liang Wang","Li Cheng","Guoying Zhao"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"4572da0671c181666e2612cb17652a89","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/igi-book-09/","publishdate":"2020-06-22T11:05:44.252708Z","relpermalink":"/publication/igi-book-09/","section":"publication","summary":"","tags":null,"title":"Machine Learning for Human Motion Analysis: Theory and Practice","type":"publication"},{"authors":["Li Cheng","Minglun Gong"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"61ad9f0635de05ff678aa5ae81d8f481","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-gon-iccv-09/","publishdate":"2020-06-22T11:05:44.321295Z","relpermalink":"/publication/che-gon-iccv-09/","section":"publication","summary":"","tags":null,"title":"Realtime Background Subtraction from Dynamic Scenes","type":"publication"},{"authors":["Jun Zhou","Li Cheng","Walter Bischof"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"cb30dabbd986d3eabb64d74c51aceaeb","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zho-che-bis-sv-09/","publishdate":"2020-06-22T11:05:44.293118Z","relpermalink":"/publication/zho-che-bis-sv-09/","section":"publication","summary":"","tags":null,"title":"Spatial-Temporal Modeling of Interactive Image Interpretation","type":"publication"},{"authors":["Nathan Brewer","Nianjun Liu","Li Cheng","Lei Wang"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"59f9dbb1d5eec2002fdb073882a7ffeb","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/bre-et-al-ivcnz-09/","publishdate":"2020-06-22T11:05:44.341696Z","relpermalink":"/publication/bre-et-al-ivcnz-09/","section":"publication","summary":"","tags":null,"title":"User-Driven Lossy Compression for Images and Video","type":"publication"},{"authors":["Li Cheng","S. V. N. Vishwanathan","Xinhua Zhang"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"93038b49aab1990f038c49d35e8ef894","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-vis-zha-cvpr-08/","publishdate":"2020-06-22T11:05:44.325617Z","relpermalink":"/publication/che-vis-zha-cvpr-08/","section":"publication","summary":"","tags":null,"title":"Consistent image analogies using semi-supervised learning","type":"publication"},{"authors":["Qinfeng Shi","Li Wang","Li Cheng","Alex Smola"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"f5a619c1a9e192c432d13b4b99ac28d7","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shi-wan-che-smo-cvpr-08/","publishdate":"2020-06-22T11:05:44.327156Z","relpermalink":"/publication/shi-wan-che-smo-cvpr-08/","section":"publication","summary":"","tags":null,"title":"Discriminative Human Action Segmentation and Recognition using. Semi-Markov Model","type":"publication"},{"authors":["Jun Zhou","Li Cheng","Walter Bischof"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"2a56288cd6147b96cf6abdc3b4446157","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zho-che-bis-aaai-08/","publishdate":"2020-06-22T11:05:44.32443Z","relpermalink":"/publication/zho-che-bis-aaai-08/","section":"publication","summary":"","tags":null,"title":"Prediction and Change Detection In Sequential Data for Interactive Applications","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"ebbc67861383c5df175e8313896bf31d","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-cviu-07/","publishdate":"2020-06-22T11:05:44.295436Z","relpermalink":"/publication/che-cae-cviu-07/","section":"publication","summary":"","tags":null,"title":"Bayesian stereo matching","type":"publication"},{"authors":["Jun Zhou","Li Cheng","Terry Caelli","Walter Bischof"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"797743c960b1370d3725b3d3a9efc6b2","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zho-che-cae-bis-hcii-07/","publishdate":"2020-06-22T11:05:44.344047Z","relpermalink":"/publication/zho-che-cae-bis-hcii-07/","section":"publication","summary":"","tags":null,"title":"Influence of Human Inputs on Semi-automatic Image Interpretation","type":"publication"},{"authors":["Tiberio Caetano","Li Cheng","Quoc Le","Alex Smola"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"d2267370a90d45b3514a2505ab4f9581","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/cae-che-le-smo-iccv-07/","publishdate":"2020-06-22T11:05:44.329497Z","relpermalink":"/publication/cae-che-le-smo-iccv-07/","section":"publication","summary":"","tags":null,"title":"Learning Graph Matching","type":"publication"},{"authors":["Li Cheng","S. V. N. Vishwanathan"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"0c8395769c8a6ae50bc71008125f995d","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-vis-cvpr-07/","publishdate":"2020-06-22T11:05:44.328313Z","relpermalink":"/publication/che-vis-cvpr-07/","section":"publication","summary":"","tags":null,"title":"Learning to compress images and videos","type":"publication"},{"authors":["Jun Zhou","Li Cheng","Walter Bischof"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"445d594e5dade5a6abc7d9a2bd258870","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zho-che-bis-tgrs-07/","publishdate":"2020-06-22T11:05:44.294317Z","relpermalink":"/publication/zho-che-bis-tgrs-07/","section":"publication","summary":"","tags":null,"title":"Online Learning with Novelty Detection in Human-guided Road Tracking","type":"publication"},{"authors":["Shaojun Wang","Shaomin Wang","Li Cheng","Russ Greiner","Dale Schuurmans"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"fa0ad830460e20f40f388ec79a7fdd43","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wan-wan-che-gre-sch-icgi-07/","publishdate":"2020-06-22T11:05:44.330589Z","relpermalink":"/publication/wan-wan-che-gre-sch-icgi-07/","section":"publication","summary":"","tags":null,"title":"Stochastic Analysis of Lexical and Semantic for Enhanced Structural Language Model","type":"publication"},{"authors":["Jun Zhou","Li Cheng","Walter Bischof"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"8871148efe55da9490f7fbbc27ebd762","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zho-che-bis-prrs-06/","publishdate":"2020-06-22T11:05:44.345124Z","relpermalink":"/publication/zho-che-bis-prrs-06/","section":"publication","summary":"","tags":null,"title":"A Novel Learning Approach for Semi-automatic Road Tracking","type":"publication"},{"authors":["Li Cheng","Shaojun Wang","Dale Schuurmans","Terry Caelli","S. V. N. Vishwanathan"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"05ed371337e228da4f36ea758d6d7635","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-wan-sch-cae-vis-avss-06/","publishdate":"2020-06-22T11:05:44.331684Z","relpermalink":"/publication/che-wan-sch-cae-vis-avss-06/","section":"publication","summary":"","tags":null,"title":"An Online Discriminative Approach to Background Subtraction","type":"publication"},{"authors":["Li Cheng","Terry Caelli","Arturo Sanchez-Azofeifa"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"94f5dc276d2016e416fa06736d0dca2b","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-tpami-06/","publishdate":"2020-06-22T11:05:44.296514Z","relpermalink":"/publication/che-cae-tpami-06/","section":"publication","summary":"","tags":null,"title":"Component Optimization for Image Understanding: a Bayesian Approach","type":"publication"},{"authors":["Terry Caelli","Li Cheng","Q Fang"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"22a0fa77aee05289c84e27e0ec19e8fe","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-fan-ijcra-05/","publishdate":"2020-06-22T11:05:44.297578Z","relpermalink":"/publication/che-cae-fan-ijcra-05/","section":"publication","summary":"","tags":null,"title":"Bayesian Image Understanding: From Images to Virtual Forests","type":"publication"},{"authors":["Shaojun Wang","Shaomin Wang","Russ Greiner","Dale Schuurmans","Li Cheng"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"67b76293dce0bec72b0b10070dcd20e3","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wan-wan-gre-sch-che-icml-05/","publishdate":"2020-06-22T11:05:44.334116Z","relpermalink":"/publication/wan-wan-gre-sch-che-icml-05/","section":"publication","summary":"","tags":null,"title":"Exploiting syntactic, semantic and lexical regularities in language modeling via directed Markov random fields","type":"publication"},{"authors":["Li Cheng","Feng Jiao","Dale Schuurmans","Shaojun Wang"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"04c7300166f30c7dd6548db074b045aa","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-jia-sch-wan-icml-05/","publishdate":"2020-06-22T11:05:44.33282Z","relpermalink":"/publication/che-jia-sch-wan-icml-05/","section":"publication","summary":"","tags":null,"title":"Variational Bayesian image modelling","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"3d3fa08249b909ccf96f1f32a98675aa","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-gmbv-04/","publishdate":"2020-06-22T11:05:44.347124Z","relpermalink":"/publication/che-cae-gmbv-04/","section":"publication","summary":"","tags":null,"title":"Bayesian Stereo Matching","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"40bdf110ddaab92e6fab77afd00f8a4f","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-lcvpr-04/","publishdate":"2020-06-22T11:05:44.346108Z","relpermalink":"/publication/che-cae-lcvpr-04/","section":"publication","summary":"","tags":null,"title":"Forestry Scene Geometry Estimation via Statistical Learning","type":"publication"},{"authors":["Terry Caelli","Li Cheng","Q Fang"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"7a9fa1dcd1cdc47227e738bc03fd9a45","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-vi-03-b/","publishdate":"2020-06-22T11:05:44.349207Z","relpermalink":"/publication/che-cae-vi-03-b/","section":"publication","summary":"","tags":null,"title":"A Bayesian Approach to Image Understanding: From Images to Virtual Forests","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"abcf95fb36adf5c6cefd4da67c3357ce","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-icassp-03/","publishdate":"2020-06-22T11:05:44.335433Z","relpermalink":"/publication/che-cae-icassp-03/","section":"publication","summary":"","tags":null,"title":"Doubly-MRF Stereo Matching","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"e06ef0ace45bb3eaaf9b2e3dcde41876","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-vi-03-a/","publishdate":"2020-06-22T11:05:44.348189Z","relpermalink":"/publication/che-cae-vi-03-a/","section":"publication","summary":"","tags":null,"title":"Unsupervised Image Segmentation: A Bayesian Approach","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1009843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1009843200,"objectID":"ec54fc5132c56ce117c6934676ba7156","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-icpr-02/","publishdate":"2020-06-22T11:05:44.336491Z","relpermalink":"/publication/che-cae-icpr-02/","section":"publication","summary":"","tags":null,"title":"A Trainable Hierarchical Hidden Markov Tree Model for color image Annotation","type":"publication"}]
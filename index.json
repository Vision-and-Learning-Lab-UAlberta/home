[{"authors":["licheng"],"categories":null,"content":"I am an associate professor with the Department of Electrical and Computer Engineering, University of Alberta. Prior to joining University of Alberta in year 2018, I worked at A*STAR, Singapore, TTI-Chicago, USA, and NICTA, Australia. I received my BSc degree in Computer Science from Jilin University in 1996, M. Eng. degree from Nankai University in 1999, and PhD in Computing Science from the University of Alberta in 2004. My research expertise is mainly on computer vision and machine learning.\n","date":1641254400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1641254400,"objectID":"35904e6911f33bff2955e50d3ec791a7","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/li-cheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/li-cheng/","section":"authors","summary":"I am an associate professor with the Department of Electrical and Computer Engineering, University of Alberta. Prior to joining University of Alberta in year 2018, I worked at A*STAR, Singapore, TTI-Chicago, USA, and NICTA, Australia.","tags":null,"title":"Li Cheng","type":"authors"},{"authors":["shuangwu"],"categories":null,"content":"Chuan Guo is a PhD student at the Vision and Learning Lab. His research interests include computer vision and machine learning.\n","date":1641254400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1641254400,"objectID":"2f44b728c95bf3cf134217ac2f94af6f","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/shuang-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shuang-wu/","section":"authors","summary":"Chuan Guo is a PhD student at the Vision and Learning Lab. His research interests include computer vision and machine learning.","tags":null,"title":"Shuang Wu","type":"authors"},{"authors":["jingjingli"],"categories":null,"content":"Jingjing Li is a PhD student under the supervision of Prof. Li Cheng at the Vision and Learning Lab, University of Alberta. Her work now focuses on Visual Saliency Analysis, Human Activity Understanding, and Medical Image Processing. She received the M.Sc. degree and B.S. degree from Dalian University of Technology, China, in 2019 and 2017, respectively.\n","date":1638748800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1638748800,"objectID":"0877e4db656bd39107101fc25759b677","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/jingjing-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingjing-li/","section":"authors","summary":"Jingjing Li is a PhD student under the supervision of Prof. Li Cheng at the Vision and Learning Lab, University of Alberta. Her work now focuses on Visual Saliency Analysis, Human Activity Understanding, and Medical Image Processing.","tags":null,"title":"Jingjing Li","type":"authors"},{"authors":["weiji"],"categories":null,"content":"Wei Ji is a PhD student under the supervision of Prof. Li Cheng. His work focuses on designing efficient computer vision algorithms for 3D scene understanding, visual saliency analysis, and medical image processing. For more information, please visit his personal webpage.\n","date":1638748800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1638748800,"objectID":"56d1cb33564a4b777ca985617511e538","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/wei-ji/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wei-ji/","section":"authors","summary":"Wei Ji is a PhD student under the supervision of Prof. Li Cheng. His work focuses on designing efficient computer vision algorithms for 3D scene understanding, visual saliency analysis, and medical image processing.","tags":null,"title":"Wei Ji","type":"authors"},{"authors":["jiyang"],"categories":null,"content":"Ji Yang is a PhD student with the Vision and Learning Lab at the Department of Electrical and Computer Engineering, University of Alberta. He received both his BSc Honors (2017) and MSc degree (2019) at the Department of Computing Science, University of Alberta. His research interests are mainly on computer vision, human hehavior analysis and machine learning.\n","date":1635724800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1635724800,"objectID":"a13dc716b1fa7fd3a82b8b575972f71c","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/ji-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ji-yang/","section":"authors","summary":"Ji Yang is a PhD student with the Vision and Learning Lab at the Department of Electrical and Computer Engineering, University of Alberta. He received both his BSc Honors (2017) and MSc degree (2019) at the Department of Computing Science, University of Alberta.","tags":null,"title":"Ji Yang","type":"authors"},{"authors":["senwang"],"categories":null,"content":"Sen Wang is a Postdoctoral Fellow under the supervision of Dr. Li Cheng at University of Alberta and Dr. Minglun Gong at University of Guelph. He have graduated from the Northwestern Polytechnical University with his Ph.D degree in 2019, advised by Dr. Runxiao Wang. Before that, he recieved my B.Eng. degree from the School of Mechanical Engineering from Northwestern Polytechnical University in 2011. During 2015-2016, he was a Visiting Ph.D Student at University of Kentucky, advised by Dr. Ruigang Yang. His research interests include Computer Vision and Robotics, in particular in Motion Retargeting and Human Modeling.\nMy personal website can be visited here.\n","date":1635724800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1635724800,"objectID":"3626cece87f3467111c0fcd63a1ed522","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/sen-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sen-wang/","section":"authors","summary":"Sen Wang is a Postdoctoral Fellow under the supervision of Dr. Li Cheng at University of Alberta and Dr. Minglun Gong at University of Guelph. He have graduated from the Northwestern Polytechnical University with his Ph.","tags":null,"title":"Sen Wang","type":"authors"},{"authors":["xinxinzuo"],"categories":null,"content":"Xinxin Zuo is Postdoctoral Fellow under the supervision of Prof. Li Cheng at University of Alberta and Prof. Minglun Gong at University of Guelph. She have graduated from the University of Kentucky with her Ph.D degree in 2019, advised by Prof. Ruigang Yang. Before that, She recieved my B.Eng. and M.Eng. degree from the School of Computer Science at Northwestern Polytechnical University in 2011 and 2014. Her research interests include Computer Vision and Computer Graphics, in particular in 3D Reconstruction and Human Modeling.\nMy personal website can be visited here.\n","date":1635724800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1635724800,"objectID":"13020e93f889b0f7b71da0f13efdf970","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/xinxin-zuo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xinxin-zuo/","section":"authors","summary":"Xinxin Zuo is Postdoctoral Fellow under the supervision of Prof. Li Cheng at University of Alberta and Prof. Minglun Gong at University of Guelph. She have graduated from the University of Kentucky with her Ph.","tags":null,"title":"Xinxin Zuo","type":"authors"},{"authors":["youdongma"],"categories":null,"content":"Youdong Ma obtained his BSc in Computing Science from University of Alberta in 2016. He is a first-year master student with Vision and Learning Lab at the Department of Electrical and Computer Engineering, University of Alberta. His research interest is computer vision and human behavior analysis.\n","date":1635724800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1635724800,"objectID":"2587919ac91f367247b9f9de2fd93580","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/youdong-ma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/youdong-ma/","section":"authors","summary":"Youdong Ma obtained his BSc in Computing Science from University of Alberta in 2016. He is a first-year master student with Vision and Learning Lab at the Department of Electrical and Computer Engineering, University of Alberta.","tags":null,"title":"Youdong Ma","type":"authors"},{"authors":["khaghanijavad"],"categories":null,"content":"","date":1632614400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1632614400,"objectID":"f4087ff9395c74f9a37656ba3bb863e9","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/javad-khaghani/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/javad-khaghani/","section":"authors","summary":"","tags":null,"title":"Javad Khaghani","type":"authors"},{"authors":["mahdiarn"],"categories":null,"content":"","date":1632614400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1632614400,"objectID":"9adc0dc4aaccbc7d876db03b03c3743c","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/mahdiar-nekoui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mahdiar-nekoui/","section":"authors","summary":"","tags":null,"title":"Mahdiar Nekoui","type":"authors"},{"authors":["mojtabas"],"categories":null,"content":"The focus of my research is on visual object tracking.\n Digital Image \u0026amp; Video Processing Lab (DIVPL), Yazd University, Iran Image Processing Lab (IPL), Sharif University of Technology, http://ipl.ce.sharif.edu/members.html  ","date":1632614400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1632614400,"objectID":"8fca144910d17d7118b4cd503093d653","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/seyed-mojtaba-marvasti-zadeh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/seyed-mojtaba-marvasti-zadeh/","section":"authors","summary":"The focus of my research is on visual object tracking.\n Digital Image \u0026amp; Video Processing Lab (DIVPL), Yazd University, Iran Image Processing Lab (IPL), Sharif University of Technology, http://ipl.ce.sharif.edu/members.html  ","tags":null,"title":"Seyed Mojtaba Marvasti-Zadeh","type":"authors"},{"authors":["chuanguo"],"categories":null,"content":"Chuan Guo is a Phd student under the supervision of Dr.Li Cheng. His work now focuses on modeling human motions, such as multi-modal human motion generation. He had two-year research experience in the Multimedia Group at Institute of Computing Technology, Chinese Academy of Sciences. Before that, he recieved his bachelor degree from Software Engineering College at Jilin University. For more information, please visit his personal webpage.\n","date":1630022400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1630022400,"objectID":"142ed72b86ee96c93fea999fe151f5c6","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/chuan-guo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chuan-guo/","section":"authors","summary":"Chuan Guo is a Phd student under the supervision of Dr.Li Cheng. His work now focuses on modeling human motions, such as multi-modal human motion generation. He had two-year research experience in the Multimedia Group at Institute of Computing Technology, Chinese Academy of Sciences.","tags":null,"title":"Chuan Guo","type":"authors"},{"authors":["hoangn"],"categories":null,"content":"Hoang is a MSc student with the Vision and Learning Lab at the Department of Electrical and Computer Engineering, University of Alberta. His research interests are mainly on computer vision, medical imaging, natural language processing, and machine learning.\n","date":1630022400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1630022400,"objectID":"f6ef5d5a10d8cf3598ac03320460b2fd","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/nguyen-tran-nhat-hoang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nguyen-tran-nhat-hoang/","section":"authors","summary":"Hoang is a MSc student with the Vision and Learning Lab at the Department of Electrical and Computer Engineering, University of Alberta. His research interests are mainly on computer vision, medical imaging, natural language processing, and machine learning.","tags":null,"title":"Nguyen Tran Nhat Hoang","type":"authors"},{"authors":["shihaozou"],"categories":null,"content":"I am a second-year PhD student in the department of Electrical and Computer Engineering, University of Alberta, Canada. I am from Nanchang, China. My research interest includes computer vision (CV) and reinforcement learning (RL). I also participated in projects on multi-agent RL and some data mining topics, including information retrieval and natural language processing. My homepage\n","date":1630022400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1630022400,"objectID":"f8275347c56279c737988751172c0c96","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/shihao-zou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shihao-zou/","section":"authors","summary":"I am a second-year PhD student in the department of Electrical and Computer Engineering, University of Alberta, Canada. I am from Nanchang, China. My research interest includes computer vision (CV) and reinforcement learning (RL).","tags":null,"title":"Shihao Zou","type":"authors"},{"authors":["tk"],"categories":null,"content":"I am a masters student at the University of Alberta with an interest in Machine Learning, and Computer Vision. I obtained my BSc from Tel Aviv University in 2019, where I worked with Professor Nachum Dershowitz on various Digital Humanities projects (see publications). In 2019, I joined the University of Alberta as a masters student, supervised by Professor Li Cheng. I am currently working on medical report generation, and video summarization. When I am not working, I like running, making jokes, and playing video games (usually not simultaneously).\n Past publications before my master study：\n  Badamdorj, Taivanbat, Adiel Ben-Shalom, and Nachum Dershowitz. \u0026ldquo;Matching and Searching the Dead Sea Scrolls.\u0026rdquo; 2018 IEEE International Conference on the Science of Electrical Engineering in Israel (ICSEE). IEEE, 2018.\n  Badamdorj, Taivanbat and Ben-Shalom, Adiel and Dershowitz, Nachum and Wolf, Lior. \u0026ldquo;Fast Search with Poor OCR\u0026rdquo; Abstracts of Digital Humanities. DH 2020, Ottawa, Canada\n  ","date":1630022400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1630022400,"objectID":"fb75d4c2070e898f9491360f071501f5","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/taivanbat-badamdorj/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/taivanbat-badamdorj/","section":"authors","summary":"I am a masters student at the University of Alberta with an interest in Machine Learning, and Computer Vision. I obtained my BSc from Tel Aviv University in 2019, where I worked with Professor Nachum Dershowitz on various Digital Humanities projects (see publications).","tags":null,"title":"Taivanbat Badamdorj","type":"authors"},{"authors":["heyan"],"categories":null,"content":"He Yan received the master’s degree from Nanjing Forestry University, Jiangsu, China, in 2014. Currently, he is pursuing the Ph.D. degree at Nanjing University of Science and Technology. His research interests include machine learning, pattern recognition, intelligent transportation systems, and their applications.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e2059829264e82c23e6a3f38db343bcc","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/he-yan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/he-yan/","section":"authors","summary":"He Yan received the master’s degree from Nanjing Forestry University, Jiangsu, China, in 2014. Currently, he is pursuing the Ph.D. degree at Nanjing University of Science and Technology. His research interests include machine learning, pattern recognition, intelligent transportation systems, and their applications.","tags":null,"title":"He Yan","type":"authors"},{"authors":["lizifu"],"categories":null,"content":"Lizi Fu is a first year MSc student under the supervision of Dr. Li Cheng and Dr. Xingyu Li at the Department of Electrical and Computer Engineering, University of Alberta. She received her BEng degree (2021) at School of Intelligent Systems Engineering, Sun Yat-sen University. Lizi comes from Chengdu, China. Her research interests are mainly on machine learning, computer vision and 3D reconstruction. For more information, please visit her personal page here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"52b0d7a75f860933dd15e1954a0716d3","permalink":"https://vision-and-learning-lab-ualberta.github.io/author/lizi-fu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/lizi-fu/","section":"authors","summary":"Lizi Fu is a first year MSc student under the supervision of Dr. Li Cheng and Dr. Xingyu Li at the Department of Electrical and Computer Engineering, University of Alberta. She received her BEng degree (2021) at School of Intelligent Systems Engineering, Sun Yat-sen University.","tags":null,"title":"Lizi Fu","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]]\rname = \u0026quot;Courses\u0026quot;\rurl = \u0026quot;courses/\u0026quot;\rweight = 50\r Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]]\rname = \u0026quot;Docs\u0026quot;\rurl = \u0026quot;docs/\u0026quot;\rweight = 50\r Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://vision-and-learning-lab-ualberta.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://vision-and-learning-lab-ualberta.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://vision-and-learning-lab-ualberta.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":["Zhenguang Liu","Shuang Wu","Shuyuan Jin","Qi Liu","Shouling Ji","Shijian Lu","Li Cheng"],"categories":null,"content":"","date":1641254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641254400,"objectID":"a6841af959a61bad80dfbb59f58bae1f","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shuang_investigating_2022/","publishdate":"2022-01-04T00:00:00Z","relpermalink":"/publication/shuang_investigating_2022/","section":"publication","summary":"Predicting human motion from historical pose sequence is crucial for a machine to succeed in intelligent interactions with humans. One aspect that has been obviated so far, is the fact that how we represent the skeletal pose has a critical impact on the prediction results. Yet there is no effort that investigates across different pose representation schemes. We conduct an indepth study on various pose representations with a focus on their effects on the motion prediction task. Moreover, recent approaches build upon off-the-shelf RNN units for motion prediction. These approaches process input pose sequence sequentially and inherently have difficulties in capturing long-term dependencies. In this paper, we propose a novel RNN architecture termed AHMR for motion prediction which simultaneously models local motion contexts and a global context. We further explore a geodesic loss and a forward kinematics loss, which have more geometric significance than the widely employed L2 loss. Interestingly, we applied our method to a range of articulate objects including human, fish, and mouse. Empirical results show that our approach outperforms the state-of-the-art methods in short-term prediction and achieves much enhanced long-term prediction proficiency, such as retaining natural human-like motions over 50 seconds predictions. Our codes are released.","tags":["TPAMI"],"title":"Investigating Pose Representations and Motion Contexts Modeling for 3D Motion Prediction","type":"publication"},{"authors":["Zhenguang Liu","Shuang Wu","Shuyuan Jin","Qi Liu","Shouling Ji","Shijian Lu","Li Cheng"],"categories":null,"content":"","date":1641254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641254400,"objectID":"4741dd414d2d9a750fee7aa94773e7a9","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/shuang_tpami2022/","publishdate":"2022-01-04T00:00:00Z","relpermalink":"/post/shuang_tpami2022/","section":"post","summary":"","tags":null,"title":"Our paper \"Investigating Pose Representations and Motion Contexts Modeling for 3D Motion Prediction\" is accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence!","type":"post"},{"authors":["Jingjing Li","Wei Ji","Qi Bi","Cheng Yan","Miao Zhang","Yongri Piao","Huchuan Lu","Li Cheng"],"categories":null,"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"daace30f52745e5b3aa6ef627ce43186","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/jingjing_joint_2021/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/publication/jingjing_joint_2021/","section":"publication","summary":"Training saliency detection models with weak supervisions, e.g., image-level tags or captions, is appealing as it removes the costly demand of per-pixel annotations. Despite the rapid progress of RGB-D saliency detection in fully-supervised setting, it however remains an unexplored territory when only weak supervision signals are available. This paper is set to tackle the problem of weakly-supervised RGB-D salient object detection. The key insight in this effort is the idea of maintaining per-pixel pseudo-labels with iterative refinements by reconciling the multimodal input signals in our joint semantic mining (JSM). Considering the large variations in the raw depth map and the lack of explicit pixel-level supervisions, we propose spatial semantic modeling (SSM) to capture saliency-specific depth cues from the raw depth and produce depth-refined pseudo-labels. Moreover, tags and captions are incorporated via a fill-in-the-blank training in our textual semantic modeling (TSM) to estimate the confidences of competing pseudo-labels. At test time, our model involves only a light-weight sub-network of the training pipeline, i.e., it requires only an RGB image as input, thus allowing efficient inference. Extensive evaluations demonstrate the effectiveness of our approach under the weakly-supervised setting. Importantly, our method could also be adapted to work in both fully-supervised and unsupervised paradigms. In each of these scenarios, superior performance has been attained by our approach with comparing to the state-of-the-art dedicated methods. As a by-product, a CapS dataset is constructed by augmenting existing benchmark training set with additional image tags and captions.","tags":["NeurIPS"],"title":"Joint Semantic Mining for Weakly Supervised RGB-D Salient Object Detection","type":"publication"},{"authors":["Jingjing Li","Wei Ji","Qi Bi","Cheng Yan","Miao Zhang","Yongri Piao","Huchuan Lu","Li Cheng"],"categories":null,"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"3f7a7c74f312d151d31b841964b4ac5f","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/jingjing_neurips2021/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/post/jingjing_neurips2021/","section":"post","summary":"","tags":null,"title":"Our paper \"Joint Semantic Mining for Weakly Supervised RGB-D Salient Object Detection\" is accepted by NeurIPS 2021","type":"post"},{"authors":["Ji Yang","Youdong Ma","Xinxin Zuo","Sen Wang","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"9ac2122e7e0c3c0910c839dbb9bdf4ef","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/ji_3d_2021/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/publication/ji_3d_2021/","section":"publication","summary":"This paper considers to jointly tackle the highly correlated tasks of estimating 3D human body poses and predicting future 3D motions from RGB image sequences. Based on Lie algebra pose representation, a novel self-projection mechanism is proposed that naturally preserves human motion kinematics. This is further facilitated by a sequence-to-sequence multi-task architecture based on an encoder-decoder topology, which enables us to tap into the common ground shared by both tasks. Finally, a global refinement module is proposed to boost the performance of our framework. The effectiveness of our approach, called PoseMoNet, is demonstrated by ablation tests and empirical evaluations on Human3.6M and HumanEva-I benchmark, where competitive performance is obtained comparing to the state-of-the-arts.","tags":["PR"],"title":"3D Pose Estimation and Future Motion Prediction from 2D Images","type":"publication"},{"authors":["Ji Yang","Youdong Ma","Xinxin Zuo","Sen Wang","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"72c40c9667a37aac7f6421efb00112e5","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/ji_pr_2021/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/post/ji_pr_2021/","section":"post","summary":"This paper considers to jointly tackle the highly correlated tasks of estimating 3D human body poses and predicting future 3D motions from RGB image sequences.","tags":null,"title":"Our paper \"3D pose estimation and future motion prediction from 2D images\" is accepted by Pattern Recogntion","type":"post"},{"authors":["Seyed Mojtaba Marvasti-Zadeh","Javad Khaghani","Li Cheng","Hossein Ghanei-Yakhdan","Shohreh Kasaei"],"categories":null,"content":"","date":1632614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632614400,"objectID":"f5bc64fbf699cefaa1ec145ed5c1dde0","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/mojtaba_chase_2021/","publishdate":"2021-09-26T00:00:00Z","relpermalink":"/publication/mojtaba_chase_2021/","section":"publication","summary":"A strong visual object tracker nowadays relies on its well-crafted modules, which typically consist of manually-designed network architectures to deliver high-quality tracking results. Not surprisingly, the manual design process becomes a particularly challenging barrier, as it demands sufficient prior experience, enormous effort, intuition, and perhaps some good luck. Meanwhile, neural architecture search has gaining grounds in practical applications as a promising method in tackling the issue of automated search of feasible network structures. In this work, we propose a novel cell-level differentiable architecture search mechanism with early stopping to automate the network design of the tracking module, aiming to adapt backbone features to the objective of Siamese tracking networks during offline training. Besides, the proposed early stopping strategy avoids over-fitting and performance collapse problems leading to generalization improvement. The proposed approach is simple, efficient, and with no need to stack a series of modules to construct a network. Our approach is easy to be incorporated into existing trackers, which is empirically validated using different differentiable architecture search-based methods and tracking objectives. Extensive experimental evaluations demonstrate the superior performance of our approach over five commonly-used benchmarks.","tags":["BMVC"],"title":"CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search","type":"publication"},{"authors":["Mahdiar Nekoui","Li Cheng"],"categories":null,"content":"","date":1632614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632614400,"objectID":"6ad176cf642badebaae8ed559e93ee19","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/mahdiar_enhancing_2021/","publishdate":"2021-09-26T00:00:00Z","relpermalink":"/publication/mahdiar_enhancing_2021/","section":"publication","summary":"The space of human motions is vast, ranging from daily behaviors of healthy adults to the slow and stiff motions of Parkinson's patients, or to infant motions. This poses significant challenges when the task is focused on a relatively niche motion subspace such as physical rehabilitation: often the target datasets are limited and less-annotated; meanwhile, there exist large-scale, well-annotated benchmarks, typically consisting of daily activities from healthy adults. This observation inspires us to propose a two-stage pipeline that takes advantage of the best of both worlds: a non-expert network starts to learn the representation of normal motions from source datasets, by estimating the pace and a set of manually inpainted joints of the pose sequence; this is followed by an expert network that takes as input these representations as well as the appearance features of the dedicated motions from the target dataset, to assess the quality of the specific actions. Empirical experiments on two very different motion assessment applications (physical rehabilitation of Parkinson's \u0026 stroke patients, and neuromotor behaviors of infants) demonstrate the superior performance of our approach.","tags":["BMVC"],"title":"Enhancing Human Motion Assessment by Self-supervised Representation Learning","type":"publication"},{"authors":["Seyed Mojtaba Marvasti-Zadeh","Javad Khaghani","Li Cheng","Hossein Ghanei-Yakhdan","Shohreh Kasaei"],"categories":null,"content":"","date":1632614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632614400,"objectID":"aab5a1f75a702c493c618bbfcd3e03a3","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/mojtaba_bmvc2021/","publishdate":"2021-09-26T00:00:00Z","relpermalink":"/post/mojtaba_bmvc2021/","section":"post","summary":"A strong visual object tracker nowadays relies on its well-crafted modules, which typically consist of manually-designed network architectures to deliver high-quality tracking results. Not surprisingly, the manual design process becomes a particularly challenging barrier, as it demands sufficient prior experience, enormous effort, intuition, and perhaps some good luck. Meanwhile, neural architecture search has gaining grounds in practical applications as a promising method in tackling the issue of automated search of feasible network structures. In this work, we propose a novel cell-level differentiable architecture search mechanism with early stopping to automate the network design of the tracking module, aiming to adapt backbone features to the objective of Siamese tracking networks during offline training. Besides, the proposed early stopping strategy avoids over-fitting and performance collapse problems leading to generalization improvement. The proposed approach is simple, efficient, and with no need to stack a series of modules to construct a network. Our approach is easy to be incorporated into existing trackers, which is empirically validated using different differentiable architecture search-based methods and tracking objectives. Extensive experimental evaluations demonstrate the superior performance of our approach over five commonly-used benchmarks.","tags":null,"title":"Our paper \"CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search\" is accepted by BMVC 2021","type":"post"},{"authors":["Mahdiar Nekoui","Li Cheng"],"categories":null,"content":"","date":1632614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632614400,"objectID":"e1b9813fd4e4f53a0acd0f94251b4450","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/mahdiar_bmvc2021/","publishdate":"2021-09-26T00:00:00Z","relpermalink":"/post/mahdiar_bmvc2021/","section":"post","summary":"","tags":null,"title":"Our paper \"Enhancing Human Motion Assessment by Self-supervised Representation Learning\" is accepted by BMVC 2021","type":"post"},{"authors":["Nguyen Tran Nhat Hoang","Dong Nie","Taivanbat Badamdorj","Yujie Liu","Jason Truong","Li Cheng"],"categories":null,"content":"","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630022400,"objectID":"8ce16f962224713a881f61cdd52da18c","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/hoang_automated_emnlp/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/publication/hoang_automated_emnlp/","section":"publication","summary":"Our paper focuses on automating the generation of medical reports from chest X-ray image inputs, a critical yet time-consuming task for radiologists. Unlike existing medical re-port generation efforts that tend to produce human-readable reports, we aim to generate medical reports that are both fluent and clinically accurate. This is achieved by our fully differentiable and end-to-end paradigm containing three complementary modules: taking the chest X-ray images and clinical his-tory document of patients as inputs, our classification module produces an internal check-list of disease-related topics, referred to as enriched disease embedding; the embedding representation is then passed to our transformer-based generator, giving rise to the medical reports; meanwhile, our generator also pro-duces the weighted embedding representation, which is fed to our interpreter to ensure consistency with respect to disease-related topics.Our approach achieved promising results on commonly-used metrics concerning language fluency and clinical accuracy. Moreover, noticeable performance gains are consistently ob-served when additional input information is available, such as the clinical document and extra scans of different views.","tags":["EMNLP","Text Generation"],"title":"Automated Generation of Accurate and Fluent Medical X-ray Reports","type":"publication"},{"authors":["Shihao Zou","Chuan Guo","Xinxin Zuo","Sen Wang","Pengyu Wang","Xiaoqin Hu","Shoushun Chen","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630022400,"objectID":"e12094533bb7d832dd8d3a541cd500aa","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shihao_eventhpe_2021/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/publication/shihao_eventhpe_2021/","section":"publication","summary":"Event camera is an emerging imaging sensor for capturing dynamics of moving objects as events, which motivates our work in estimating 3D human pose and shape from the event signals. Events, on the other hand, have their unique challenges: rather than capturing static body postures, the event signals are best at capturing local motions. This leads us to propose a two-stage deep learning approach, called EventHPE. The first-stage, FlowNet, is trained by unsupervised learning to infer optical flow from events. Both events and optical flow are closely related to human body dynamics, which are fed as input to the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate the discrepancy between image-based flow (optical flow) and shape-based flow (vertices movement of human body shape), a novel flow coherence loss is introduced by exploiting the fact that both flows are originated from the identical human motion. An in-house event-based 3D human dataset is curated that comes with 3D pose and shape annotations, which is by far the largest one to our knowledge. Empirical evaluations on DHP19 dataset and our in-house dataset demonstrate the effectiveness of our approach.","tags":["ICCV"],"title":"EventHPE: Event-based 3D Human Pose and Shape Estimation","type":"publication"},{"authors":["Taivanbat Badamdorj","Mrigank Rochan","Yang Wang","Li Cheng"],"categories":null,"content":"","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630022400,"objectID":"c3229d5bfbc5389ccb4444ca3004130c","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/tk_joint_2021/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/publication/tk_joint_2021/","section":"publication","summary":"In video highlight detection, the goal is to identify the interesting moments within an unedited video. Although the audio component of the video provides important cues for highlight detection, the majority of existing efforts focus almost exclusively on the visual component. In this paper, we argue that both audio and visual components of a video should be modeled jointly to retrieve its best moments. To this end, we propose an audio-visual network for video highlight detection. At the core of our approach lies a bimodal attention mechanism, which captures the interaction between the audio and visual components of a video, and produces fused representations to facilitate highlight detection. Furthermore, we introduce a noise sentinel technique to adaptively discount a noisy visual or audio modality. Empirical evaluations on two benchmark datasets demonstrate the superior performance of our approach over the state-of-the-art methods.","tags":["ICCV","Highlight Detection"],"title":"Joint Visual and Audio Learning for Video Highlight Detection","type":"publication"},{"authors":["Nguyen Tran Nhat Hoang","Dong Nie","Taivanbat Badamdorj","Yujie Liu","Yingying Zhu","Jason Truong","Li Cheng"],"categories":null,"content":"","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630022400,"objectID":"239a630cbfc110c6d7bc49c36ffeb94f","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/hoang_emnlp2021/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/post/hoang_emnlp2021/","section":"post","summary":" ","tags":null,"title":"Our paper \"Automated Generation of Accurate \u0026 Fluent Medical X-ray Reports\" is accepted by EMNLP 2021","type":"post"},{"authors":["Shuang Wu","Zhenguang Liu","Shijian Lu","Li Cheng"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"75226946b7b150147d96c6f47cc859a0","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shuangwu_dual_2021/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/publication/shuangwu_dual_2021/","section":"publication","summary":"Music and dance have always co-existed as pillars of human activities, contributing immensely to the cultural, social, and entertainment functions in virtually all societies. Notwithstanding the gradual systematization of music and dance into two independent disciplines, their intimate connection is undeniable and one art-form often appears incomplete without the other. Recent research works have studied generative models for dance sequences conditioned on music. The dual task of composing music for given dances, however, has been largely overlooked. In this paper, we propose a novel extension, where we jointly model both tasks in a dual learning approach. To leverage the duality of the two modalities, we introduce an optimal transport objective to align feature embeddings, as well as a cycle consistency loss to foster overall consistency. Experimental results demonstrate that our dual learning framework improves individual task performance, delivering generated music compositions and dance choreographs that are realistic and faithful to the conditioned inputs.","tags":["MM"],"title":"Dual Learning Music Composition and Dance Choreography","type":"publication"},{"authors":["Shuang Wu","Zhenguang Liu","Shijian Lu","Li Cheng"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"2de0a8c9c0ced2928e9b2fe1e3f0d383","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/shuang_acmmm2021/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/post/shuang_acmmm2021/","section":"post","summary":" ","tags":null,"title":"Our paper \"Dual Learning Music Composition and Dance Choreography\" is accepted by ACM Multimedia 2021","type":"post"},{"authors":["Shihao Zou","Chuan Guo","Xinxin Zuo","Sen Wang","Pengyu Wang","Xiaoqin Hu","Shoushun Chen","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"2c8b2883695907aeaa1c2370ae1f21d6","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/shihao_iccv2021/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/post/shihao_iccv2021/","section":"post","summary":" ","tags":null,"title":"Our paper \"EventHPE: Event-based 3-D Human Pose Estimation\" is accepted by ICCV 2021","type":"post"},{"authors":["Taivanbat Badamdorj","Mrigank Rochan","Yang Wang","Li Cheng"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"ad64b0b5097571419e42715ce7d8ed03","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/tk_iccv2021/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/post/tk_iccv2021/","section":"post","summary":" ","tags":null,"title":"Our paper \"Joint Visual and Audio Learning for Video Highlight Detection\" is accepted by ICCV 2021","type":"post"},{"authors":["Wei Ji","Jingjing Li","Shuang Yu","Miao Zhang","Yongri Piao","Shunyu Yao","Qi Bi","Kai Ma","Yefeng Zheng","Huchuan Lu","Li Cheng"],"categories":null,"content":"","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"2b4b5e68e32bf11f914e812d694aa087","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/wei_cvpr_2021a/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/post/wei_cvpr_2021a/","section":"post","summary":"This paper systematically addresses the depth-related side effects via the designed calibration strategy towards boosting saliency detection accuracy.","tags":null,"title":"Our paper \"Calibrated RGB-D Salient Object Detection\" is accepted by IEEE Conference on Computer Vision and Pattern Recognition 2021","type":"post"},{"authors":["Wei Ji","Shuang Yu","Junde Wu","Kai Ma","Cheng Bian","Qi Bi","Jingjing Li","Hanruo Liu","Li Cheng","Yefeng Zheng"],"categories":null,"content":"","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"9ed4a9a2a9cb45c4405cefc6824ae43e","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/wei_cvpr_2021b/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/post/wei_cvpr_2021b/","section":"post","summary":"This paper proposes a principled research investigation on exploiting the rich agreement information among multiple raters for improving the calibrated performance.","tags":null,"title":"Our paper \"Learning Calibrated Medical Image Segmentation via Multi-rater Agreement Modeling\" is accepted by IEEE Conference on Computer Vision and Pattern Recognition 2021 (Best Paper Candidate)","type":"post"},{"authors":["Wei Ji","Jingjing Li","Shuang Yu","Miao Zhang","Yongri Piao","Shunyu Yao","Qi Bi","Kai Ma","Yefeng Zheng","Huchuan Lu","Li Cheng"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"806baf62fd136911f272556d2dedd828","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wei_cvpr21_dcf/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/wei_cvpr21_dcf/","section":"publication","summary":"This paper systematically addresses the depth-related side effects via the designed calibration strategy towards boosting saliency detection accuracy.","tags":["CVPR"],"title":"Calibrated RGB-D Salient Object Detection","type":"publication"},{"authors":["Wei Ji","Shuang Yu","Junde Wu","Kai Ma","Cheng Bian","Qi Bi","Jingjing Li","Hanruo Liu","Li Cheng","Yefneg Zhang"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"3d8cbe1eae0f7df3f68f82c3ee4a42df","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wei_cvpr21_mrnet/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/wei_cvpr21_mrnet/","section":"publication","summary":"This paper proposes a principled research investigation on exploiting the rich agreement information among multiple raters for improving the calibrated performance.","tags":["CVPR"],"title":"Learning Calibrated Medical Image Segmentation via Multi-rater Agreement Modeling","type":"publication"},{"authors":["Seyed Mojtaba Marvasti-Zadeh","Li Cheng","Hossein Ghanei-Yakhdan","Shohreh Kasaei"],"categories":null,"content":"","date":1606435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606435200,"objectID":"586137e0de033f7721f35b40d7b78d12","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/mojtaba_tits2020/","publishdate":"2020-11-27T00:00:00Z","relpermalink":"/post/mojtaba_tits2020/","section":"post","summary":"","tags":null,"title":"Our paper \"Deep Learning for Visual Tracking: A Comprehensive Survey\" is accepted by IEEE Transactions on Intelligent Transportation Systems (T-ITS)","type":"post"},{"authors":["Chuan Guo","Xinxin Zuo","Sen Wang","Shihao Zou","Qingyao Sun","Annan Deng","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"6516898824f6ccdac9759c900ecfc801","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/chuan_action2motion_mm/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/chuan_action2motion_mm/","section":"publication","summary":"A temporal VAE archtecture model equipped with Lie Algebra representation for action-conditioned 3D human motion generation.","tags":["MM","Motion Generation"],"title":"Action2Motion: Conditioned Generation of 3D Human Motions","type":"publication"},{"authors":["Chuan Guo","Xinxin Zuo","Sen Wang","Shihao Zou","Qingyao Sun","Annan Deng","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1595635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595635200,"objectID":"3ef6bb36c36513693a3b5b554d09d17c","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/chuan_acmmm2020/","publishdate":"2020-07-25T00:00:00Z","relpermalink":"/post/chuan_acmmm2020/","section":"post","summary":" ","tags":null,"title":"Our paper \"Action2Motion: Conditioned Generation of 3-D Human Motions\" is accepted by ACM Multimedia 2020","type":"post"},{"authors":["Shihao Zou","Xinxin Zuo","Sen Wang","Li Cheng"],"categories":null,"content":"","date":1593820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593820800,"objectID":"60fd10ee83d313d543686c13fcca26d3","permalink":"https://vision-and-learning-lab-ualberta.github.io/post/zou_eccv2020/","publishdate":"2020-07-04T00:00:00Z","relpermalink":"/post/zou_eccv2020/","section":"post","summary":" ","tags":null,"title":"Our paper \"3D Human Shape Reconstruction from a Polarization Image\" is accepted by ECCV 2020","type":"post"},{"authors":["Shihao Zou","Xinxin Zuo","Yiming Qian","Sen Wang","Chi Xu","Minglun Gong","Li Cheng"],"categories":null,"content":"\rClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.\r\r\r\rClick the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.\r\r\rSupplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --\r","date":1593648000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593648000,"objectID":"8552f1b3ed1ccd24d1d946c2f940bfd1","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shihaozou_polarization_2020/","publishdate":"2020-07-02T00:00:00Z","relpermalink":"/publication/shihaozou_polarization_2020/","section":"publication","summary":"This paper tackles a new problem of estimating 3D body shape of clothed humans from single polarized 2D images, i.e. a polarization image.","tags":["ECCV"],"title":"3D Human Shape Reconstruction from a Polarization Image","type":"publication"},{"authors":["Mahdiar Nekoui","Fidel Omar Tito Cruz","Li Cheng"],"categories":null,"content":"","date":1592524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592524800,"objectID":"670a8a94f576b2ff0f0f4efb5dc9b453","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/mahdiar_falcons_2020/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/mahdiar_falcons_2020/","section":"publication","summary":"Isn't it about time to help judges with the challenging task of evaluating athletes' performances in sports with extreme poses? To tackle this problem and inspired by human judges' grading schema, we propose a virtual refereeing network to evaluate the execution of a diving performance. This assessment would be based on visual clues as well as the body joints sequence of the action video. In order to cover the unusual body contortions in such scenarios, we present ExPose: annotated dataset of Extreme Poses. We further introduce a simple yet effective module to assess the difficulty of the performance based on the extracted joints sequence. Finally, the overall score of the performance would be reported as the multiplication of the execution and difficulty scores. The results demonstrate our proposed lightweight network not only achieves state-of-the-art results compared to previous studies in diving but also shows acceptable generalization to other contortive sports.","tags":["CVPR","CVPRW"],"title":"FALCONS: FAst Learner-grader for CONtorted poses in Sports","type":"publication"},{"authors":["Xinxin Zuo","Sen Wang","Jiangbin Zheng","Weiwei Yu","Minglun Gong","Ruigang Yang","Li Cheng"],"categories":null,"content":"","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591747200,"objectID":"dfd06bfcf83548aba78abc0b07d14732","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xinxin_tmm_2020/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/publication/xinxin_tmm_2020/","section":"publication","summary":"A novel approach to reconstruct 3D human body shapes based on a sparse set of RGBD frames using a single RGBD camera","tags":["TMM"],"title":"SparseFusion: Dynamic Human Avatar Modeling from Sparse RGBD Images","type":"publication"},{"authors":["Ji Yang","Youdong Ma","Li Cheng"],"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"b23808b5539c562cda8fb8b481fff4c2","permalink":"https://vision-and-learning-lab-ualberta.github.io/project/jiyang_multitask_2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/project/jiyang_multitask_2020/","section":"project","summary":"A multitask learning system for 3D pose esitimation and future motion prediction from video.","tags":["Deep Learning","Pose Estimation","Motion Prediction"],"title":"3D Pose Estimation and Future Motion Prediction from 2D Images","type":"project"},{"authors":["A. Banerjeea","S. Wu","Li Cheng","S. Aw"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"11891018b89e96395217352e7a1cc61b","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/ban-et-al-jo-ve-20/","publishdate":"2020-06-22T11:05:44.337563Z","relpermalink":"/publication/ban-et-al-jo-ve-20/","section":"publication","summary":"","tags":null,"title":"Fully automated leg movement tracking in freely moving insects using Feature Learning Leg Segmentation and Tracking (FLLIT)","type":"publication"},{"authors":["P. Porwal","S. Pachade","M. Kokare","et al."],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"3902ddd851b531e5742c6791d35f1d10","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/lyu-et-al-mia-20/","publishdate":"2020-06-22T11:05:44.254436Z","relpermalink":"/publication/lyu-et-al-mia-20/","section":"publication","summary":"","tags":null,"title":"IDRiD: Diabetic Retinopathy - Segmentation and Grading Challenge","type":"publication"},{"authors":["He Zhao","Huiqi Li","Li Cheng"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"46ab12980ee65e32440e802237ffc6f9","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-pr-20/","publishdate":"2020-06-22T11:05:44.255024Z","relpermalink":"/publication/zha-et-al-pr-20/","section":"publication","summary":"","tags":null,"title":"Improving retinal vessel segmentation with joint local loss by matting","type":"publication"},{"authors":["D. Teng","X. Zhang","Li Cheng","Delin Chu"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"4b8e629432ff5b1aec2edac4efebc3fb","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/ten-et-al-tbd-20/","publishdate":"2020-06-22T11:05:44.253734Z","relpermalink":"/publication/ten-et-al-tbd-20/","section":"publication","summary":"","tags":null,"title":"Least Squares Approximation via Sparse Subsampled Randomized Hadamard Transform","type":"publication"},{"authors":["Shuang Wu","Zhenguang Liu","Shuyuan Jin","Qi Liu","Shijian Lu","Roger Zimmermann","Li Cheng"],"categories":null,"content":"\rClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.\r\r\r\rClick the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.\r\r\rSupplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --\r","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"2a84508ad3130d7bcd6062250fb085b5","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shuangwu_hmr_2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/shuangwu_hmr_2019/","section":"publication","summary":"A hierarchical recurrent network structure is developed to simultaneously encodes local contexts of individual frames and global contexts of the sequence.","tags":["CVPR"],"title":"Towards Natural and Accurate Future Motion Prediction of Humans and Animals","type":"publication"},{"authors":["Shuang Wu","Zhenguang Liu","Shuyuan Jin","Qi Liu","Shijian Lu","Roger Zimmermann","Li Cheng"],"categories":null,"content":"","date":1556323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556323200,"objectID":"545a19981a73ac7fa09f37b55cef1da4","permalink":"https://vision-and-learning-lab-ualberta.github.io/project/shuangwu_hmr_2019/","publishdate":"2019-04-27T00:00:00Z","relpermalink":"/project/shuangwu_hmr_2019/","section":"project","summary":"Towards natural and accurate future motion prediction of humans and animals.","tags":["Deep Learning","Motion Prediction"],"title":"Hierarchical Motion Recurrent Network for Future Motion Prediction of Humans and Animals","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic \rAcademic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click \rPDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? \rAsk\n\rDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://vision-and-learning-lab-ualberta.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["S. Wu","K. Tan","L. Govindarajan","J. Stewart","H. Zhu","L. Gu","M. Katarya","B. Wong","E. Tan","D. Li","A. Chang","C. Libedinsky","L. Cheng","S. Aw"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3e32d70ceb0a4b4f0c1a3009e20d24d1","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wu-et-al-plos-bio-19/","publishdate":"2020-06-22T11:05:44.25635Z","relpermalink":"/publication/wu-et-al-plos-bio-19/","section":"publication","summary":"","tags":null,"title":"Fully automated leg tracking of Drosophila neurodegeneration models reveals distinct conserved movement signatures","type":"publication"},{"authors":["Xiaowei Zhang","Xudong Shi","Yu Sun","Li Cheng"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"571d49abeef592e0e8a9fdb4ef70b42d","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-tpami-19/","publishdate":"2020-06-22T11:05:44.25559Z","relpermalink":"/publication/zha-et-al-tpami-19/","section":"publication","summary":"","tags":null,"title":"Multivariate Regression with Gross Errors on Manifold-valued Data","type":"publication"},{"authors":["Qi Liu","Yue Zhang","Ye Yuan","Zhenguang Liu","Li Cheng","Roger Zimmermann"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"11a39798d5ff2d76dbfe7ac12f6424dc","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/liu-et-al-aaai-18/","publishdate":"2020-06-22T11:05:44.298671Z","relpermalink":"/publication/liu-et-al-aaai-18/","section":"publication","summary":"","tags":null,"title":"Multi-modal Multi-task Learning for Automatic Dietary Assessment","type":"publication"},{"authors":["He Zhao","Huiqi Li","Sebastian Maurer-Stroh","Yuhong Guo","Qiuju Deng","Li Cheng"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"7e0d96354751b82102a25c1e1eab5802","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-tmi-18/","publishdate":"2020-06-22T11:05:44.257173Z","relpermalink":"/publication/zha-et-al-tmi-18/","section":"publication","summary":"","tags":null,"title":"Supervised Segmentation of Un-annotated Retinal Fundus Images by Synthesis","type":"publication"},{"authors":["He Zhao","Huiqi Li","Sebastian Maurer-Stroh","Li Cheng"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"03b1eecc59ba0893dbf01e6e450d731e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-med-ia-18/","publishdate":"2020-06-22T11:05:44.257789Z","relpermalink":"/publication/zha-et-al-med-ia-18/","section":"publication","summary":"","tags":null,"title":"Synthesizing Retinal and Neuronal Images with Generative Adversarial Nets","type":"publication"},{"authors":["X. Zhang","L. Cheng","B. Li","Hai-Miao Hu"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d82f20796ab170cb4b9ce5f0480a5f6e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-tip-18/","publishdate":"2020-06-22T11:05:44.258366Z","relpermalink":"/publication/zha-et-al-tip-18/","section":"publication","summary":"","tags":null,"title":"Too Far to See? Not Really! Pedestrian Detection with Scale-aware Localization Policy","type":"publication"},{"authors":["J. De","X. Zhang","F. Lin","L. Cheng"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"552e26bbae97598652181a7c05bd1012","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/de-et-al-tpami-18/","publishdate":"2020-06-22T11:05:44.258979Z","relpermalink":"/publication/de-et-al-tpami-18/","section":"publication","summary":"","tags":null,"title":"Transduction on Directed Graphs via Absorbing Random Walks","type":"publication"},{"authors":["Z. Liu","L. Zhang","Q. Liu","Y. Yin","L. Cheng","R. Zimmermann"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"fc8480a63324950d61b154c2398d542e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/liu-et-al-tmm-17/","publishdate":"2020-06-22T11:05:44.265416Z","relpermalink":"/publication/liu-et-al-tmm-17/","section":"publication","summary":"","tags":null,"title":"Fusion of Magnetic and Vision sensors for indoor localization: Infrastructure-free and More Effective","type":"publication"},{"authors":["C. Xu","L. Govindarajan","L. Cheng"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"8e22d2ea911eb00a3732823a654b3603","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xu-gov-che-pr-17/","publishdate":"2020-06-22T11:05:44.259655Z","relpermalink":"/publication/xu-gov-che-pr-17/","section":"publication","summary":"","tags":null,"title":"Hand Action Detection from Ego-centric Depth Sequences","type":"publication"},{"authors":["Ch. Xu","L. Govindarajan","Y. Zhang","L. Cheng"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"2e03cee8ed5a8d5f0ae5a90cee510234","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xu-et-al-ijcv-17/","publishdate":"2020-06-22T11:05:44.261455Z","relpermalink":"/publication/xu-et-al-ijcv-17/","section":"publication","summary":"","tags":null,"title":"Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking, and Action Recognition on Lie Groups","type":"publication"},{"authors":["Zhenguang Liu","L. Cheng","Anan Liu","Luming Zhang","Xiangnan He","Roger Zimmermann"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"ceff61c53181010dba85e7e5303f8f19","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/liu-et-al-mm-17/","publishdate":"2020-06-22T11:05:44.299757Z","relpermalink":"/publication/liu-et-al-mm-17/","section":"publication","summary":"","tags":null,"title":"Multiview and Multimodal Pervasive Indoor Localization","type":"publication"},{"authors":["Ch. Xu","L. Zhang","L. Cheng","R. Koch"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"b7b413d24d6c8921f7d465dc48701337","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xu-et-al-tpami-17/","publishdate":"2020-06-22T11:05:44.262471Z","relpermalink":"/publication/xu-et-al-tpami-17/","section":"publication","summary":"","tags":null,"title":"Pose Estimation from Line Correspondences: A Complete Analysis and A Series of Solutions","type":"publication"},{"authors":["A. Cliffe","D. Doupe","H. Sung","L. Hwee","L. Cheng","K. Ong","W. Yu"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"e968ecb9ee98294495a15e5b93d8198f","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/cli-et-al-nc-17/","publishdate":"2020-06-22T11:05:44.26048Z","relpermalink":"/publication/cli-et-al-nc-17/","section":"publication","summary":"","tags":null,"title":"Quantitative 3D analysis of complex single border cell behaviors in coordinated collective cell migration","type":"publication"},{"authors":["H. Tie","B. Chen","X. Sun","L. Cheng","L. Lu"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"8ac9f8f0a8d1cbc0496e189abf202c46","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/tie-et-al-jo-ve-17/","publishdate":"2020-06-22T11:05:44.338633Z","relpermalink":"/publication/tie-et-al-jo-ve-17/","section":"publication","summary":"","tags":null,"title":"Quantitative localization of a Golgi protein by imaging its fluorescence center of mass","type":"publication"},{"authors":["G. Lin","X. Zhang","H. Zhao","H. Li","L. Cheng"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f557646c0300cae4e2ea2510bd524387","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gu-et-al-tmi-17/","publishdate":"2020-06-22T11:05:44.264337Z","relpermalink":"/publication/gu-et-al-tmi-17/","section":"publication","summary":"","tags":null,"title":"Segment 2D and 3D Filaments by Learning Structured and Contextual Features","type":"publication"},{"authors":["Li Cheng"],"categories":null,"content":"Joining us\u0026hellip; The Vision \u0026amp; Learning Lab at the ECE Dept. University of Alberta, is inviting outstanding Postdoc candidates to join us.\nWe are setting up a new lab at the ECE Dept., University of Alberta, focusing on exciting research topics in computer vision and machine learning. We are looking for exceptional Postdocs to join us.\nFurther information about the PI, Dr. Li Cheng, can be found below, or via https://www.ece.ualberta.ca/~lcheng5/ or by direct inquiries to lcheng5@ualberta.ca.\nPotential candidates are requested to email their CVs (in PDF) to Li Cheng (\rlcheng5@ualberta.ca).\nAbout the PI\u0026hellip; Li CHENG is an associate professor with the ECE Dept., University of Alberta, Canada. His research expertise is mainly in computer vision and machine learning, with application focus in both visual behavior analysis and biomedical image analysis. His research work has resulted in over 90 referred papers including those published at journals such as IEEE Trans. Pattern Analysis and Machine Intelligence, International Journal of Computer Vision, as well as conferences such as ICML, NIPS, ICCV, CVPR, MICCAI, and AAAI. He is a senior member of IEEE.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"7cc43f76fb802bdd6f3dba71478b786e","permalink":"https://vision-and-learning-lab-ualberta.github.io/archive/opening_postdoc/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/archive/opening_postdoc/","section":"archive","summary":" ","tags":["Recruiting"],"title":"","type":"archive"},{"authors":["J. De","L. Cheng","X. Zhang","F. Lin","H. Li","K. Ong","W. Yu","Y. Yu","S. Ahmed"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"0a270a7876518a81ca76879405493622","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/de-et-al-tmi-16/","publishdate":"2020-06-22T11:05:44.271519Z","relpermalink":"/publication/de-et-al-tmi-16/","section":"publication","summary":"","tags":null,"title":"A Graph-theoretical Approach for Tracing Filamentary Structures in Neuronal and Retinal Images","type":"publication"},{"authors":["H. Tie","D. Mahajan","B. Chen","L. Cheng","A. VanDongen","L. Lu"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"24fc22998ffe65d9b3b0d943fd6a36a3","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/tie-et-al-m-bo-c-16/","publishdate":"2020-06-22T11:05:44.269271Z","relpermalink":"/publication/tie-et-al-m-bo-c-16/","section":"publication","summary":"","tags":null,"title":"A novel imaging method for quantitative Golgi localization reveals differential intra-Golgi trafficking of secretory cargos","type":"publication"},{"authors":["Y. Zhang","L. Cheng","J. Wu","J. Cai","Mi. Do","J. Lu"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"447d1554d98dc2e50c0ea91257e5b42e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-tip-16/","publishdate":"2020-06-22T11:05:44.266533Z","relpermalink":"/publication/zha-et-al-tip-16/","section":"publication","summary":"","tags":null,"title":"Action Recognition in Still Images with Minimum Annotation Efforts","type":"publication"},{"authors":["C. Xu","A. Nanjappa","X. Zhang","L. Cheng"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"7a01a6f825531ffb0b3cc1289a5f8510","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xu-et-al-ijcv-16/","publishdate":"2020-06-22T11:05:44.270387Z","relpermalink":"/publication/xu-et-al-ijcv-16/","section":"publication","summary":"","tags":null,"title":"Estimate Hand Poses Efficiently from Single Depth Images","type":"publication"},{"authors":["X. Zhang","L. Cheng","D. Chu","L. Liao","M. Ng","R. Tan"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"f549739d92a2c0f29ffa8a5c02760bbf","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-sisc-16/","publishdate":"2020-06-22T11:05:44.263212Z","relpermalink":"/publication/zha-et-al-sisc-16/","section":"publication","summary":"","tags":null,"title":"Incremental Regularized Least Squares for Dimensionality Reduction of Large-Scale Data","type":"publication"},{"authors":["K. Ong","J. De","L. CHENG","S. AHMED","W. YU"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"5f7318a8c58c42861a8ef0004fa861c5","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/ong-et-al-cyto-16/","publishdate":"2020-06-22T11:05:44.26779Z","relpermalink":"/publication/ong-et-al-cyto-16/","section":"publication","summary":"","tags":null,"title":"NeuronCyto II: An Automatic and Quantitative Solution for Crossover Neural Cells in High Throughput Screening","type":"publication"},{"authors":["Li Liu","Li Cheng","Ye Liu","Yongpo Jia","David S. Rosenblum"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"ab46e053f97ae942da9a941dfdb98a8f","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/liu-et-al-aaai-16/","publishdate":"2020-06-22T11:05:44.300848Z","relpermalink":"/publication/liu-et-al-aaai-16/","section":"publication","summary":"","tags":null,"title":"Recognizing Complex Activities by a Probabilistic Interval-based Model","type":"publication"},{"authors":["Yiming Qian","Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"df17b013bb6c45e7dfffefa7193a2ff4","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/qia-gon-che-cai-15/","publishdate":"2020-06-22T11:05:44.305156Z","relpermalink":"/publication/qia-gon-che-cai-15/","section":"publication","summary":"","tags":null,"title":"An Efficient Self-Tuning Multiclass Classification Approach","type":"publication"},{"authors":["C. Yap","E. Kalaw","M. Singh","K. Chong","D. Giron","C. Huang","L. Cheng","Y. Law","H. Lee"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"b453890df6f78ee064da10794f2f0220","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/yap-et-al-jpi-15/","publishdate":"2020-06-22T11:05:44.273834Z","relpermalink":"/publication/yap-et-al-jpi-15/","section":"publication","summary":"","tags":null,"title":"Automated Image Based Prominent Nucleoli Detection","type":"publication"},{"authors":["Ashwin Nanjappa","Chi Xu","Li Cheng"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"e33357c5bfd594b8a0f4651b45ace217","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/nan-chi-che-eurographics-15/","publishdate":"2020-06-22T11:05:44.304079Z","relpermalink":"/publication/nan-chi-che-eurographics-15/","section":"publication","summary":"","tags":null,"title":"GHand: A GPU algorithm for realtime hand pose estimation using depth camera","type":"publication"},{"authors":["M. Gong","Y. Qian","L. Cheng"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"c7562d1345aadb3f03d8e8b4156c58a1","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gon-qia-che-tip-15/","publishdate":"2020-06-22T11:05:44.272652Z","relpermalink":"/publication/gon-qia-che-tip-15/","section":"publication","summary":"","tags":null,"title":"Integrated Foreground Segmentation and Boundary Matting for Live Videos","type":"publication"},{"authors":["Lin Gu","Li Cheng"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"e01d1698b96d566a0e23b02e9636c30b","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gu-che-iccv-15/","publishdate":"2020-06-22T11:05:44.303057Z","relpermalink":"/publication/gu-che-iccv-15/","section":"publication","summary":"","tags":null,"title":"Learning to Boost Filamentary Structure Segmentation","type":"publication"},{"authors":["Xiaowei Zhang","Li Cheng","Tingshao Zhu"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"7a67c3cd24cd4c29059055e207906790","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-che-zhu-acml-15/","publishdate":"2020-06-22T11:05:44.302008Z","relpermalink":"/publication/zha-che-zhu-acml-15/","section":"publication","summary":"","tags":null,"title":"Robust Multivariate Regression with Grossly Corrupted Observations and Its Application to Personality Prediction","type":"publication"},{"authors":["Meiguang Jin","Lakshmi Govindarajan","L. Cheng"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"7b27085996bdc122fd10d3fd44e14b38","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/jin-et-al-isbi-14/","publishdate":"2020-06-22T11:05:44.307353Z","relpermalink":"/publication/jin-et-al-isbi-14/","section":"publication","summary":"","tags":null,"title":"A Random-Forest Random Field Approach for Cellular Image Segmentation","type":"publication"},{"authors":["Jia Zhang","Huiqi Li","Qing Nie","Li Cheng"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"5fdbe21def8bcaef7d9bdbe6a0cdc41c","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zha-et-al-cmig-14/","publishdate":"2020-06-22T11:05:44.279667Z","relpermalink":"/publication/zha-et-al-cmig-14/","section":"publication","summary":"","tags":null,"title":"A retinal vessel boundary tracking method based on Bayesian theory and multi-scale line detection","type":"publication"},{"authors":["K. Yong","T. Gong","M. Nongpiur","A. How","H. Lee","L. Cheng","S. Perera","T. Aung"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"71aea105d043b9485bc54407c079764e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/yon-et-al-optho-14/","publishdate":"2020-06-22T11:05:44.278501Z","relpermalink":"/publication/yon-et-al-optho-14/","section":"publication","summary":"","tags":null,"title":"Myopia in Asian Subjects with Primary Angle Closure: Implications for Glaucoma Trends in East Asia","type":"publication"},{"authors":["T. Thi","L. Wang","J. Zhang","S. Maurer-Stroh","L. Cheng"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"bcf2eb55b524c5d3822583b2a321a43e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-et-al-bmcbioinfo-14/","publishdate":"2020-06-22T11:05:44.277425Z","relpermalink":"/publication/thi-et-al-bmcbioinfo-14/","section":"publication","summary":"","tags":null,"title":"Recognizing Flu-like Symptoms from Videos","type":"publication"},{"authors":["Li Cheng","Sinno Jialin Pan"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"bb049cffd713d732134973aff98876de","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-pan-tnnls-14/","publishdate":"2020-06-22T11:05:44.274967Z","relpermalink":"/publication/che-pan-tnnls-14/","section":"publication","summary":"","tags":null,"title":"Semi-supervised Domain Adaptation on Manifolds","type":"publication"},{"authors":["Li Cheng","Jaydeep De","Xiaowei Zhang","Feng Lin","Huiqi Li"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"462de3588e5790ed294c8275fc309fe9","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-et-al-miccai-14/","publishdate":"2020-06-22T11:05:44.306289Z","relpermalink":"/publication/che-et-al-miccai-14/","section":"publication","summary":"","tags":null,"title":"Tracing Retinal Blood Vessels by Matrix-Forest Theorem of Directed Graphs","type":"publication"},{"authors":["Jaydeep De","Huiqi Li","Li Cheng"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"de5fb8c09ddbc1b43e8fb1ddbc590a6a","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/de-li-che-bmcbioinfo-14/","publishdate":"2020-06-22T11:05:44.276096Z","relpermalink":"/publication/de-li-che-bmcbioinfo-14/","section":"publication","summary":"","tags":null,"title":"Tracing retinal vessel trees by transductive inference","type":"publication"},{"authors":["C. Guzman","T. Gong","M. Nongpiur","S. Perera","A. How","H. Lee","L. Cheng","M. He","M. Baskaran","T. Aung"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"7d00e522f70559e8949dd055afe3e4c0","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/guz-et-al-iovs-13/","publishdate":"2020-06-22T11:05:44.284382Z","relpermalink":"/publication/guz-et-al-iovs-13/","section":"publication","summary":"","tags":null,"title":"Anterior segment optical coherence tomography parameters in subtypes of primary angle closure","type":"publication"},{"authors":["Jaydeep De","Tengfei Ma","Huiqi Li","M. Dash","Li Cheng"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"7f100743faa4ac8b6efe3ecaec79e806","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/jay-et-al-scia-13/","publishdate":"2020-06-22T11:05:44.339665Z","relpermalink":"/publication/jay-et-al-scia-13/","section":"publication","summary":"","tags":null,"title":"Automated Tracing of Retinal Blood Vessels Using Graphical Models","type":"publication"},{"authors":["Matti Pietikainen","Matthew Turk","Liang Wang","Guoyin Zhao","Li Cheng"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"7125bee57911f3ca31ce5ecb430fab1c","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/pie-et-al-ivc-13/","publishdate":"2020-06-22T11:05:44.28083Z","relpermalink":"/publication/pie-et-al-ivc-13/","section":"publication","summary":"","tags":null,"title":"Editorial of the Special issue: Machine Learning in Motion Analysis: New Advances","type":"publication"},{"authors":["Chi Xu","Li Cheng"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"0fbbc47a3cf8250ced941303eff64a19","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/xu-che-iccv-13/","publishdate":"2020-06-22T11:05:44.308441Z","relpermalink":"/publication/xu-che-iccv-13/","section":"publication","summary":"","tags":null,"title":"Efficient Hand Pose Estimation from a Single Depth Image","type":"publication"},{"authors":["Shaojun Wang","Shaomin Wang","Li Cheng","Russell Greiner","Dale Schuurman"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"d697c66b9565d3c468a85ea23042dd0f","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wan-et-al-ci-13/","publishdate":"2020-06-22T11:05:44.281846Z","relpermalink":"/publication/wan-et-al-ci-13/","section":"publication","summary":"","tags":null,"title":"Exploiting Syntactic, Semantic, and Lexical Regularities in Language Modeling via Directed Markov Random Fields","type":"publication"},{"authors":["Tianxia Gong","Li Cheng Nengli Lim","Hwee Kuan Lee","Bolan Su","Shimiao Li","Chew Lim Tan","Boon Chuan Pang","C. C. Tchoyoson Lim","Cheng Kiang Lee"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"ad1e5df72c9837317233d4fa8cca0548","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gon-et-al-ictai-13/","publishdate":"2020-06-22T11:05:44.311341Z","relpermalink":"/publication/gon-et-al-ictai-13/","section":"publication","summary":"","tags":null,"title":"Finding Distinctive Shape Features for Hematoma Classification in Brain CT Images","type":"publication"},{"authors":["Li Cheng"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"06f5ab303cbea1e398ad4026b7212c6b","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-icml-13/","publishdate":"2020-06-22T11:05:44.310327Z","relpermalink":"/publication/che-icml-13/","section":"publication","summary":"","tags":null,"title":"Riemannian Similarity Learning","type":"publication"},{"authors":["M. Nongpiur","T. Gong","H. Lee","S. Perera","L. Cheng","L. Foo","M. He","D. Friedman","T. Aung"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"6a7525eea9d939bc9295e0d068068181","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/non-et-al-optho-13/","publishdate":"2020-06-22T11:05:44.282988Z","relpermalink":"/publication/non-et-al-optho-13/","section":"publication","summary":"","tags":null,"title":"Subgrouping of Primary Angle-Closure Suspects Based on Anterior Segment Optical Coherence Tomography Parameters","type":"publication"},{"authors":["Li Cheng","Ning Ye","Weimiao Yu","Andre Cheah"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"b46f4814956cf1f3c30280b52c8c5216","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-et-al-springerbkchapt-12/","publishdate":"2020-06-22T11:05:44.340699Z","relpermalink":"/publication/che-et-al-springerbkchapt-12/","section":"publication","summary":"","tags":null,"title":"A Bag-of-Words Model for Cellular Image Segmentation","type":"publication"},{"authors":["Tuan Hue Thi","Li Cheng","Jian Zhang","Li Wang","Shinichi Satoh"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"34c34ad3697beaee3bf378bcf9c82a72","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-et-al-cviu-12/","publishdate":"2020-06-22T11:05:44.286217Z","relpermalink":"/publication/thi-et-al-cviu-12/","section":"publication","summary":"","tags":null,"title":"Integrating Local Action Elements for Action Analysis","type":"publication"},{"authors":["Tuan Hue Thi","Li Cheng","Jian Zhang","Li Wang","Shinichi Satoh"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"616627fb9317c85b54c3446c86ce7b27","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-et-al-ivc-12/","publishdate":"2020-06-22T11:05:44.287368Z","relpermalink":"/publication/thi-et-al-ivc-12/","section":"publication","summary":"","tags":null,"title":"Structured learning of local features for human action classification and localization","type":"publication"},{"authors":["Li Cheng","Ning Ye","Weimiao Yu","Andre Cheah"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"b567d835880598d4ef7bcc9d56de5412","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-et-al-miccai-11/","publishdate":"2020-06-22T11:05:44.312474Z","relpermalink":"/publication/che-et-al-miccai-11/","section":"publication","summary":"","tags":null,"title":"Discriminative Cellular Segmentation for Microscopic Images","type":"publication"},{"authors":["Qinfeng Shi","Li Wang","Li Cheng","Alex Smola"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"f0693617850063796af08b29b72387a3","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shi-et-al-ijcv-11/","publishdate":"2020-06-22T11:05:44.290469Z","relpermalink":"/publication/shi-et-al-ijcv-11/","section":"publication","summary":"","tags":null,"title":"Discriminative Human Action Segmentation and Recognition using SMMs","type":"publication"},{"authors":["Li Wang","Li Cheng","Li Wang"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"2ca7e891bbea008d55a2d587fde815a9","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wan-che-wan-tip-11/","publishdate":"2020-06-22T11:05:44.289428Z","relpermalink":"/publication/wan-che-wan-tip-11/","section":"publication","summary":"","tags":null,"title":"Elastic Sequence Correlation for Human Action Analysis","type":"publication"},{"authors":["Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"80601a8f9170c6d1e558856a17f04670","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gon-che-cvpr-11/","publishdate":"2020-06-22T11:05:44.313469Z","relpermalink":"/publication/gon-che-cvpr-11/","section":"publication","summary":"","tags":null,"title":"Foreground Segmentation of Live Videos using Locally Competing 1SVMs","type":"publication"},{"authors":["Minglun Gong","Li Cheng"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"f677554c8791f9823dab099e516062b4","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/gon-che-icip-11/","publishdate":"2020-06-22T11:05:44.314434Z","relpermalink":"/publication/gon-che-icip-11/","section":"publication","summary":"","tags":null,"title":"Incorporating Estimated Motion in Real-time Background Subtraction","type":"publication"},{"authors":["Li Cheng","Minglun Gong","Dale Schuurmans","Terry Caelli"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"13c92323c1b0a800b9e2d670a555944e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-et-al-tip-11/","publishdate":"2020-06-22T11:05:44.288413Z","relpermalink":"/publication/che-et-al-tip-11/","section":"publication","summary":"","tags":null,"title":"Real-time Discriminative Background Subtraction","type":"publication"},{"authors":["Ke Jia","Li Cheng","Nianjun Liu","Li Wang"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"7b4c11ec6e7327a4a99994e5bfcc055e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/jia-che-liu-wan-icpr-10/","publishdate":"2020-06-22T11:05:44.316613Z","relpermalink":"/publication/jia-che-liu-wan-icpr-10/","section":"publication","summary":"","tags":null,"title":"Efficient Learning to Label Images","type":"publication"},{"authors":["Tuan Hue Thi","Li Cheng","Jian Zhang","Li Wang","Shinichi Satoh"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"97a1772938234256609fc1bbfd645bf7","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-che-zha-wan-sat-avss-10/","publishdate":"2020-06-22T11:05:44.320234Z","relpermalink":"/publication/thi-che-zha-wan-sat-avss-10/","section":"publication","summary":"","tags":null,"title":"Human Action Recognition and Localization in Video using Structured Learning of Local Space-Time Features","type":"publication"},{"authors":["Li Wang","Li Cheng","Tuan Hue Thi","Jian Zhang"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"25c901d51d5ca04fa5bff8f5303fb64a","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wan-che-thi-zha-dicta-10/","publishdate":"2020-06-22T11:05:44.315447Z","relpermalink":"/publication/wan-che-thi-zha-dicta-10/","section":"publication","summary":"","tags":null,"title":"Human Action Recognition from Boosted Pose Estimation","type":"publication"},{"authors":["Tuan Hue Thi","Li Cheng","Jian Zhang","Li Wang"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"3c6a5f79d611d44cb85d7650ac426e9e","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-che-zha-wan-icpr-10/","publishdate":"2020-06-22T11:05:44.319191Z","relpermalink":"/publication/thi-che-zha-wan-icpr-10/","section":"publication","summary":"","tags":null,"title":"Implicit Motion-Shape Model: A generic approach for action matching","type":"publication"},{"authors":["Tiberio Caetano","Julian McAuley","Li Cheng","Quoc Le","Alex Smola"],"categories":null,"content":"","date":1243814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1243814400,"objectID":"8b54e7867a12a56acd783704b3ba98f4","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/cae-et-al-tpami-09/","publishdate":"2020-06-22T11:05:44.291744Z","relpermalink":"/publication/cae-et-al-tpami-09/","section":"publication","summary":"","tags":null,"title":"Learning Graph Matching","type":"publication"},{"authors":["Tuan Hue Thi","Sijun Lu","Jian Zhang","Li Cheng","Li Wang"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"5060980fb27e086c56866f23f2559129","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/thi-et-al-avss-09/","publishdate":"2020-06-22T11:05:44.323362Z","relpermalink":"/publication/thi-et-al-avss-09/","section":"publication","summary":"","tags":null,"title":"Human Body Articulation for Action Recognition in Video Sequences","type":"publication"},{"authors":["Yuxi Li","Li Cheng","Dale Schuurmans"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"a13b6403abb83d196f57d01e5fa24675","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/li-che-sch-cife-09/","publishdate":"2020-06-22T11:05:44.342988Z","relpermalink":"/publication/li-che-sch-cife-09/","section":"publication","summary":"","tags":null,"title":"Inference of the Structural Credit Risk Model using MLE","type":"publication"},{"authors":["Baochun Bai","Li Cheng","Cheng Lei","Pierre Boulanger","Janelle Harms"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"faaa14d7121583ac90bc32d8f47f08cb","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/bao-et-al-pcs-09/","publishdate":"2020-06-22T11:05:44.322355Z","relpermalink":"/publication/bao-et-al-pcs-09/","section":"publication","summary":"","tags":null,"title":"Learning-based multiview video coding","type":"publication"},{"authors":["Liang Wang","Li Cheng","Guoying Zhao"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"4572da0671c181666e2612cb17652a89","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/igi-book-09/","publishdate":"2020-06-22T11:05:44.252708Z","relpermalink":"/publication/igi-book-09/","section":"publication","summary":"","tags":null,"title":"Machine Learning for Human Motion Analysis: Theory and Practice","type":"publication"},{"authors":["Li Cheng","Minglun Gong"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"61ad9f0635de05ff678aa5ae81d8f481","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-gon-iccv-09/","publishdate":"2020-06-22T11:05:44.321295Z","relpermalink":"/publication/che-gon-iccv-09/","section":"publication","summary":"","tags":null,"title":"Realtime Background Subtraction from Dynamic Scenes","type":"publication"},{"authors":["Jun Zhou","Li Cheng","Walter Bischof"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"cb30dabbd986d3eabb64d74c51aceaeb","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zho-che-bis-sv-09/","publishdate":"2020-06-22T11:05:44.293118Z","relpermalink":"/publication/zho-che-bis-sv-09/","section":"publication","summary":"","tags":null,"title":"Spatial-Temporal Modeling of Interactive Image Interpretation","type":"publication"},{"authors":["Nathan Brewer","Nianjun Liu","Li Cheng","Lei Wang"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"59f9dbb1d5eec2002fdb073882a7ffeb","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/bre-et-al-ivcnz-09/","publishdate":"2020-06-22T11:05:44.341696Z","relpermalink":"/publication/bre-et-al-ivcnz-09/","section":"publication","summary":"","tags":null,"title":"User-Driven Lossy Compression for Images and Video","type":"publication"},{"authors":["Li Cheng","S. V. N. Vishwanathan","Xinhua Zhang"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"93038b49aab1990f038c49d35e8ef894","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-vis-zha-cvpr-08/","publishdate":"2020-06-22T11:05:44.325617Z","relpermalink":"/publication/che-vis-zha-cvpr-08/","section":"publication","summary":"","tags":null,"title":"Consistent image analogies using semi-supervised learning","type":"publication"},{"authors":["Qinfeng Shi","Li Wang","Li Cheng","Alex Smola"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"f5a619c1a9e192c432d13b4b99ac28d7","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/shi-wan-che-smo-cvpr-08/","publishdate":"2020-06-22T11:05:44.327156Z","relpermalink":"/publication/shi-wan-che-smo-cvpr-08/","section":"publication","summary":"","tags":null,"title":"Discriminative Human Action Segmentation and Recognition using. Semi-Markov Model","type":"publication"},{"authors":["Jun Zhou","Li Cheng","Walter Bischof"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"2a56288cd6147b96cf6abdc3b4446157","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zho-che-bis-aaai-08/","publishdate":"2020-06-22T11:05:44.32443Z","relpermalink":"/publication/zho-che-bis-aaai-08/","section":"publication","summary":"","tags":null,"title":"Prediction and Change Detection In Sequential Data for Interactive Applications","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"ebbc67861383c5df175e8313896bf31d","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-cviu-07/","publishdate":"2020-06-22T11:05:44.295436Z","relpermalink":"/publication/che-cae-cviu-07/","section":"publication","summary":"","tags":null,"title":"Bayesian stereo matching","type":"publication"},{"authors":["Jun Zhou","Li Cheng","Terry Caelli","Walter Bischof"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"797743c960b1370d3725b3d3a9efc6b2","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zho-che-cae-bis-hcii-07/","publishdate":"2020-06-22T11:05:44.344047Z","relpermalink":"/publication/zho-che-cae-bis-hcii-07/","section":"publication","summary":"","tags":null,"title":"Influence of Human Inputs on Semi-automatic Image Interpretation","type":"publication"},{"authors":["Tiberio Caetano","Li Cheng","Quoc Le","Alex Smola"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"d2267370a90d45b3514a2505ab4f9581","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/cae-che-le-smo-iccv-07/","publishdate":"2020-06-22T11:05:44.329497Z","relpermalink":"/publication/cae-che-le-smo-iccv-07/","section":"publication","summary":"","tags":null,"title":"Learning Graph Matching","type":"publication"},{"authors":["Li Cheng","S. V. N. Vishwanathan"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"0c8395769c8a6ae50bc71008125f995d","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-vis-cvpr-07/","publishdate":"2020-06-22T11:05:44.328313Z","relpermalink":"/publication/che-vis-cvpr-07/","section":"publication","summary":"","tags":null,"title":"Learning to compress images and videos","type":"publication"},{"authors":["Jun Zhou","Li Cheng","Walter Bischof"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"445d594e5dade5a6abc7d9a2bd258870","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zho-che-bis-tgrs-07/","publishdate":"2020-06-22T11:05:44.294317Z","relpermalink":"/publication/zho-che-bis-tgrs-07/","section":"publication","summary":"","tags":null,"title":"Online Learning with Novelty Detection in Human-guided Road Tracking","type":"publication"},{"authors":["Shaojun Wang","Shaomin Wang","Li Cheng","Russ Greiner","Dale Schuurmans"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"fa0ad830460e20f40f388ec79a7fdd43","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wan-wan-che-gre-sch-icgi-07/","publishdate":"2020-06-22T11:05:44.330589Z","relpermalink":"/publication/wan-wan-che-gre-sch-icgi-07/","section":"publication","summary":"","tags":null,"title":"Stochastic Analysis of Lexical and Semantic for Enhanced Structural Language Model","type":"publication"},{"authors":["Jun Zhou","Li Cheng","Walter Bischof"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"8871148efe55da9490f7fbbc27ebd762","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/zho-che-bis-prrs-06/","publishdate":"2020-06-22T11:05:44.345124Z","relpermalink":"/publication/zho-che-bis-prrs-06/","section":"publication","summary":"","tags":null,"title":"A Novel Learning Approach for Semi-automatic Road Tracking","type":"publication"},{"authors":["Li Cheng","Shaojun Wang","Dale Schuurmans","Terry Caelli","S. V. N. Vishwanathan"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"05ed371337e228da4f36ea758d6d7635","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-wan-sch-cae-vis-avss-06/","publishdate":"2020-06-22T11:05:44.331684Z","relpermalink":"/publication/che-wan-sch-cae-vis-avss-06/","section":"publication","summary":"","tags":null,"title":"An Online Discriminative Approach to Background Subtraction","type":"publication"},{"authors":["Li Cheng","Terry Caelli","Arturo Sanchez-Azofeifa"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"94f5dc276d2016e416fa06736d0dca2b","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-tpami-06/","publishdate":"2020-06-22T11:05:44.296514Z","relpermalink":"/publication/che-cae-tpami-06/","section":"publication","summary":"","tags":null,"title":"Component Optimization for Image Understanding: a Bayesian Approach","type":"publication"},{"authors":["Terry Caelli","Li Cheng","Q Fang"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"22a0fa77aee05289c84e27e0ec19e8fe","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-fan-ijcra-05/","publishdate":"2020-06-22T11:05:44.297578Z","relpermalink":"/publication/che-cae-fan-ijcra-05/","section":"publication","summary":"","tags":null,"title":"Bayesian Image Understanding: From Images to Virtual Forests","type":"publication"},{"authors":["Shaojun Wang","Shaomin Wang","Russ Greiner","Dale Schuurmans","Li Cheng"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"67b76293dce0bec72b0b10070dcd20e3","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/wan-wan-gre-sch-che-icml-05/","publishdate":"2020-06-22T11:05:44.334116Z","relpermalink":"/publication/wan-wan-gre-sch-che-icml-05/","section":"publication","summary":"","tags":null,"title":"Exploiting syntactic, semantic and lexical regularities in language modeling via directed Markov random fields","type":"publication"},{"authors":["Li Cheng","Feng Jiao","Dale Schuurmans","Shaojun Wang"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"04c7300166f30c7dd6548db074b045aa","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-jia-sch-wan-icml-05/","publishdate":"2020-06-22T11:05:44.33282Z","relpermalink":"/publication/che-jia-sch-wan-icml-05/","section":"publication","summary":"","tags":null,"title":"Variational Bayesian image modelling","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"3d3fa08249b909ccf96f1f32a98675aa","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-gmbv-04/","publishdate":"2020-06-22T11:05:44.347124Z","relpermalink":"/publication/che-cae-gmbv-04/","section":"publication","summary":"","tags":null,"title":"Bayesian Stereo Matching","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"40bdf110ddaab92e6fab77afd00f8a4f","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-lcvpr-04/","publishdate":"2020-06-22T11:05:44.346108Z","relpermalink":"/publication/che-cae-lcvpr-04/","section":"publication","summary":"","tags":null,"title":"Forestry Scene Geometry Estimation via Statistical Learning","type":"publication"},{"authors":["Terry Caelli","Li Cheng","Q Fang"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"7a9fa1dcd1cdc47227e738bc03fd9a45","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-vi-03-b/","publishdate":"2020-06-22T11:05:44.349207Z","relpermalink":"/publication/che-cae-vi-03-b/","section":"publication","summary":"","tags":null,"title":"A Bayesian Approach to Image Understanding: From Images to Virtual Forests","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"abcf95fb36adf5c6cefd4da67c3357ce","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-icassp-03/","publishdate":"2020-06-22T11:05:44.335433Z","relpermalink":"/publication/che-cae-icassp-03/","section":"publication","summary":"","tags":null,"title":"Doubly-MRF Stereo Matching","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"e06ef0ace45bb3eaaf9b2e3dcde41876","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-vi-03-a/","publishdate":"2020-06-22T11:05:44.348189Z","relpermalink":"/publication/che-cae-vi-03-a/","section":"publication","summary":"","tags":null,"title":"Unsupervised Image Segmentation: A Bayesian Approach","type":"publication"},{"authors":["Li Cheng","Terry Caelli"],"categories":null,"content":"","date":1009843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1009843200,"objectID":"ec54fc5132c56ce117c6934676ba7156","permalink":"https://vision-and-learning-lab-ualberta.github.io/publication/che-cae-icpr-02/","publishdate":"2020-06-22T11:05:44.336491Z","relpermalink":"/publication/che-cae-icpr-02/","section":"publication","summary":"","tags":null,"title":"A Trainable Hierarchical Hidden Markov Tree Model for color image Annotation","type":"publication"}]